{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theoretical"
      ],
      "metadata": {
        "id": "z0qMrYV49fV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What is Logistic Regression, and how does it differ from Linear Regression."
      ],
      "metadata": {
        "id": "lNF8WYtz85oH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Logistic Regression is a statistical technique used for binary classification, helping to predict\n",
        "outcomes that fall into one of two categories (such as Yes/No or 0/1). Unlike methods that predict\n",
        "continuous numerical values, Logistic Regression uses a sigmoid function to transform outputs into a\n",
        "range between 0 and 1, making it ideal for classification problems.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xn77NcB8INSF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is the mathematical equation of Logistic Regression.\n",
        "\n"
      ],
      "metadata": {
        "id": "ylJNTYtZ85q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The Logistic Regression equation is used to calculate the probability that the outcome belongs\n",
        "to the positive class (i.e., Y = 1). In this equation:\n",
        "\n",
        "\n",
        "P(Y=1)= 1/(1+e^-(B0+B1X1+B2X2+....+BnXn))\n",
        "\n",
        "\n",
        "P(Y = 1) represents the likelihood of the positive outcome.\n",
        "\n",
        "β₀ is the intercept term (a constant), and βᵢ are the weights or coefficients for each input feature.\n",
        "\n",
        "Xᵢ are the independent variables or input features used to make the prediction.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DNxPE8YoINp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Why do we use the Sigmoid function in Logistic Regression"
      ],
      "metadata": {
        "id": "Ujdgw_W885tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The sigmoid function transforms any input into a value between 0 and 1, which makes it ideal for\n",
        "classification tasks. This allows the output to be interpreted as a probability, helping to decide\n",
        "how likely it is that a given input belongs to a particular class.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0n5nsFFCIN7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.What is the cost function of Logistic Regression"
      ],
      "metadata": {
        "id": "5IXec1-385xB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Log Loss calculates the error between the predicted probability and the actual outcome.\n",
        "It assigns a higher penalty when the model is confident but wrong, making it well-suited\n",
        "for classification tasks.\n",
        "\n",
        "          m\n",
        "J(θ)=−1/m ∑ yilog(h(xi))+(1-yi)log(1-h(xi))]\n",
        "         i=1\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "3gOFZCQ1IOTC",
        "outputId": "b2f4090c-68a9-4db2-ce46-c0c762ac5783"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nLog Loss calculates the error between the predicted probability and the actual outcome. It assigns a higher penalty when the model is confident but wrong, making it well-suited for classification tasks.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.What is Regularization in Logistic Regression? Why is it needed."
      ],
      "metadata": {
        "id": "7xCnLlzu850L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Regularization helps avoid overfitting by adding an extra term to the loss function that\n",
        "penalizes large coefficients. This keeps the model simpler and more general, especially\n",
        "when working with datasets that have many features.\n",
        "\n",
        "There are two main types of regularization:\n",
        "\n",
        "Lasso (L1 Regularization): Pushes some coefficients all the way to zero, effectively removing\n",
        "less important features—so it's also useful for feature selection.\n",
        "\n",
        "Ridge (L2 Regularization): Reduces the size of the coefficients but doesn't make them exactly\n",
        "zero, helping to lower model complexity without removing features.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "JIYYreicIOpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Explain the difference between Lasso, Ridge, and Elastic Net regressionC"
      ],
      "metadata": {
        "id": "WJDsezZT853P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "-Lasso (L1 Regularization): Adds absolute penalty (λ∑∣w∣\\lambda \\sum |w|), leading to some coefficients\n",
        " being exactly zero (feature selection)'\n",
        "\n",
        "-Ridge (L2 Regularization): Adds squared penalty (λ∑w2\\lambda \\sum w^2), reducing coefficients but not\n",
        " setting them to zero\n",
        "\n",
        " -Elastic Net: A combination of L1 and L2 penalties, balancing feature selection and coefficient shrinkage.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JDJrpH_4IO-e",
        "outputId": "c3ee7510-2e9f-49eb-bd0d-ee8766a9e590"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nLasso (L1 Regularization): Adds absolute penalty (λ∑∣w∣\\\\lambda \\\\sum |w|), leading to some coefficients \\nbeing exactly zero (feature selection)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.When should we use Elastic Net instead of Lasso or Ridge"
      ],
      "metadata": {
        "id": "ZH6oL3Sg856B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Elastic Net is especially helpful in the following situations:\n",
        "\n",
        "-When the dataset has highly correlated features, since Lasso might pick one and ignore others, while\n",
        "Elastic Net can keep a balanced mix.\n",
        "\n",
        "-When you're working with high-dimensional data that includes many unimportant features—Elastic Net\n",
        "helps handle this more effectively.\n",
        "\n",
        "-When you need the benefits of both feature selection (from L1) and coefficient shrinking (from L2)\n",
        "in a single model.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "o4w79RChIPMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.What is the impact of the regularization parameter (λ) in Logistic Regression"
      ],
      "metadata": {
        "id": "LUbHXQHL859D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "- High λ\\lambda → Strong regularization, reducing overfitting but may lead to underfitting\n",
        "- Low λ\\lambda → Weak regularization, allowing more complex models, increasing overfitting risk\n",
        "- Optimal λ\\lambda is chosen via cross-validation.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "KUolI_Y1IPvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.What are the key assumptions of Logistic Regression"
      ],
      "metadata": {
        "id": "r-xDRs2685_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "-The target variable has two possible outcomes (e.g., yes/no, 0/1), which suits binary classification.\n",
        "\n",
        "-The input features should not be strongly correlated with each other (to avoid multicollinearity).\n",
        "\n",
        "-There should be a linear relationship between the independent variables and the log-odds of the\n",
        "target variable, not necessarily with the variable itself.\n",
        "\n",
        "-The classes in the dataset should be fairly balanced; if one class dominates, methods like oversampling\n",
        "or undersampling may be needed to improve model performance.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LGOKqpvCIP_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.What are some alternatives to Logistic Regression for classification tasks"
      ],
      "metadata": {
        "id": "TIM5OpMT86CU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        " There are some alternatives to Logistic Regression for classification tasks:\n",
        " Support Vector Machines (SVM\n",
        " Decision Tree\n",
        " Random Fores\n",
        " Gradient Boosting (XGBoost, LightGBM\n",
        " Neural Networks\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "E7l_jNu_IQU5",
        "outputId": "c562144a-3d2e-4325-9260-f0db02daac51"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n Decision Tree\\n Random Fores\\n Support Vector Machines (SVM\\n Gradient Boosting (XGBoost, LightGBM\\n Neural Networks\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.What are Classification Evaluation Metrics"
      ],
      "metadata": {
        "id": "a09xJTIN86FI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Common metrics used to evaluate classification models are:\n",
        "\n",
        "-Accuracy: The ratio of correctly predicted cases to the total number of predictions made.\n",
        "\n",
        "-Precision: Indicates the proportion of predicted positive cases that are truly positive.\n",
        "\n",
        "-Recall (Sensitivity): Shows the percentage of actual positive cases that the model correctly detects.\n",
        "\n",
        "-F1-score: The balance between precision and recall, calculated as their harmonic mean.\n",
        "\n",
        "-ROC-AUC: Reflects how well the model can separate the positive and negative classes across different thresholds.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lMX4v_YiIRFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.How does class imbalance affect Logistic Regression"
      ],
      "metadata": {
        "id": "5bA4uhLx9Xwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Class imbalance can cause the model to become biased toward the majority class, making it perform poorly\n",
        "on the minority class. To address this issue, you can use:\n",
        "\n",
        "-Resampling techniques, such as increasing the number of minority class samples (oversampling) or reducing\n",
        "majority class samples (undersampling).\n",
        "\n",
        "-Adjusting class weights so the model gives more importance to the minority class (e.g., by setting class_weight='balanced').\n",
        "\n",
        "-Using more suitable evaluation metrics, like the Precision-Recall curve, which are better at measuring\n",
        "performance on imbalanced data than simple accuracy.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ASf1wi7pIRb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.What is Hyperparameter Tuning in Logistic Regression"
      ],
      "metadata": {
        "id": "w14C4XKS9Xpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Hyperparameter tuning is the process of selecting the best settings for a model to improve its performance.\n",
        "For Logistic Regression, commonly adjusted hyperparameters include:\n",
        "\n",
        "-Regularization strength (λ): Controls how much the model penalizes large coefficients, affecting overfitting.\n",
        "\n",
        "-Solver: The algorithm used to optimize the model, such as 'liblinear', 'lbfgs', etc.\n",
        "\n",
        "-Penalty type: Specifies the kind of regularization to use—L1 (Lasso), L2 (Ridge), or Elastic Net—to\n",
        "control model complexity and feature selection.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vVY4WiN_IR0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.What are different solvers in Logistic Regression? Which one should be used"
      ],
      "metadata": {
        "id": "cyJ-DXd59XiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In Logistic Regression, solvers are optimization algorithms used to find the best model parameters.\n",
        "Different solvers work better depending on the dataset size, regularization type, and whether the problem\n",
        "is binary or multi-class.\n",
        "\n",
        "-liblinear → Ideal for smaller datasets and simple binary classification tasks.\n",
        "\n",
        "-lbfgs → Well-suited for handling multiclass classification problems efficiently.\n",
        "\n",
        "-saga → Versatile solver that works with L1, L2, and Elastic Net regularization, making it a good choice for\n",
        "a variety of problems.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DIDlzQwaISGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.How is Logistic Regression extended for multiclass classification"
      ],
      "metadata": {
        "id": "UjpkxGaP9Xdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Logistic Regression can be adapted for multiclass classification in two main ways:\n",
        "\n",
        "-One-vs-Rest (OvR): Creates a separate binary classifier for each class. Each model learns to distinguish\n",
        "one class from all the others, and the class with the highest score is chosen.\n",
        "\n",
        "-Softmax Regression (Multinomial Logistic Regression): A direct extension of logistic regression that\n",
        "predicts the probability of each class at once. It selects the class with the highest probability among all classes.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Cs1PrD1JIShM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.What are the advantages and disadvantages of Logistic Regression"
      ],
      "metadata": {
        "id": "qWGIY38x9XZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Advantages:\n",
        "-It's straightforward and easy to understand, making it great for interpretation and explanation.\n",
        "\n",
        "-Performs well when there's a linear relationship between the input features and the log-odds of the outcome.\n",
        "\n",
        " Disadvantages:\n",
        "-It doesn’t handle nonlinear relationships well unless you transform the features.\n",
        "\n",
        "-Assumes that the input features are independent of each other, which may not always be true in real-world data.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "yUw1ISRzISzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.What are some use cases of Logistic Regression"
      ],
      "metadata": {
        "id": "ZEWNnOGM9XTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Use Cases of Logistic Regression:\n",
        "\n",
        "-Healthcare: Used to predict the likelihood of a disease, such as determining whether a patient has diabetes or not.\n",
        "-Email Filtering: Helps identify and classify emails as spam or legitimate.\n",
        "-Financial Services: Assists in evaluating credit risk, such as deciding whether to approve or reject a loan application.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "iAjMkuolITZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.What is the difference between Softmax Regression and Logistic Regression"
      ],
      "metadata": {
        "id": "4Cyu4Ema9XNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "-Logistic Regression is designed for binary classification, where the goal is to distinguish between two classes.while,\n",
        "Softmax Regression is an extension used for multiclass classification, where it calculates the probability\n",
        "for each class and selects the one with the highest probability.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "097h4itSIT0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification"
      ],
      "metadata": {
        "id": "oZaZxh199XHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "-One-vs-Rest (OvR): Builds a separate binary classifier for each class. It's a good choice when the dataset has\n",
        " uneven class distribution or when you want a simpler and more interpretable model.\n",
        "\n",
        "-Softmax (Multinomial Logistic Regression): Predicts all class probabilities in a single model. It works best when\n",
        "the classes are fairly balanced and you need a clear probabilistic output for each class.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "U8s6RI6wIUHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.How do we interpret coefficients in Logistic Regression"
      ],
      "metadata": {
        "id": "kTpmxJLh9XBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In Logistic Regression, each coefficient (𝛽𝑖) shows how a one-unit increase in a feature affects the log-odds of the outcome:\n",
        "e^𝛽𝑖\n",
        "If 𝛽𝑖>0: The feature increases the chances of the positive class.\n",
        "\n",
        "If 𝛽𝑖<0 The feature reduces the chances of the positive class.\n",
        "\n",
        "If 𝛽𝑖=0The feature has no influence on the prediction.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "C2aqJLMnIUaG",
        "outputId": "f4c45363-92e9-40b4-be12-d503c931d2fe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIn Logistic Regression, each coefficient (𝛽𝑖) shows how a one-unit increase in a feature affects the log-odds of the outcome:\\ne^𝛽𝑖\\nIf 𝛽𝑖>0: The feature increases the chances of the positive class.\\n\\nIf 𝛽𝑖<0 The feature reduces the chances of the positive class.\\n\\nIf 𝛽𝑖=0The feature has no influence on the prediction.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# practical"
      ],
      "metadata": {
        "id": "4fXIyaI6CqYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracy"
      ],
      "metadata": {
        "id": "2mfQaOjTCqL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 3: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 4: Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and calculate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxdYB6hnGJIr",
        "outputId": "22958450-4402-4490-d786-1e8fbcba4ac5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy"
      ],
      "metadata": {
        "id": "XHYYsj29CwJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load dataset\n",
        "digits_data = load_digits()\n",
        "features = pd.DataFrame(digits_data.data)\n",
        "binary_labels = pd.Series(digits_data.target % 2)  # Convert to binary classification\n",
        "\n",
        "# Split dataset\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(\n",
        "    features, binary_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression model with L1 regularization\n",
        "logistic_model = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "logistic_model.fit(features_train, labels_train)\n",
        "\n",
        "# Predict and print classification report\n",
        "label_predictions = logistic_model.predict(features_test)\n",
        "print(\"Classification Report:\\n\", classification_report(labels_test, label_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzvwsT4TGJkg",
        "outputId": "15fbd9fa-88ac-4b9b-f648-00640882fc5e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.94      0.93       177\n",
            "           1       0.94      0.92      0.93       183\n",
            "\n",
            "    accuracy                           0.93       360\n",
            "   macro avg       0.93      0.93      0.93       360\n",
            "weighted avg       0.93      0.93      0.93       360\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficients"
      ],
      "metadata": {
        "id": "LCw3O3F6CwD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine_data = load_wine()\n",
        "features = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)\n",
        "binary_target = pd.Series(wine_data.target % 2)  # Convert to binary classification\n",
        "\n",
        "# Split dataset\n",
        "features_train, features_test, target_train, target_test = train_test_split(\n",
        "    features, binary_target, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression model with L2 regularization (Ridge)\n",
        "ridge_model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000)\n",
        "ridge_model.fit(features_train, target_train)\n",
        "\n",
        "# Predict and print results\n",
        "target_predictions = ridge_model.predict(features_test)\n",
        "print(\"Accuracy with Ridge Regularization:\", accuracy_score(target_test, target_predictions))\n",
        "print(\"Model Coefficients:\", ridge_model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EoWzWK7GKUx",
        "outputId": "53eea0c3-a097-489f-ea0b-47b6686b748e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Ridge Regularization: 0.9722222222222222\n",
            "Model Coefficients: [[-0.96354053 -1.13076443 -0.94994021  0.15273232 -0.02170646  0.21115539\n",
            "   0.54387605  0.15565509  0.57822677 -1.89792874  0.4318864   0.22922406\n",
            "  -0.0124921 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')"
      ],
      "metadata": {
        "id": "AJHttpDgCwAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "breast_cancer_data = load_breast_cancer()\n",
        "feature_data = pd.DataFrame(breast_cancer_data.data, columns=breast_cancer_data.feature_names)\n",
        "target_labels = pd.Series(breast_cancer_data.target)\n",
        "\n",
        "# Split dataset\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(\n",
        "    feature_data, target_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression with Elastic Net Regularization\n",
        "elastic_net_model = LogisticRegression(\n",
        "    penalty='elasticnet',\n",
        "    solver='saga',\n",
        "    l1_ratio=0.5,\n",
        "    max_iter=1000\n",
        ")\n",
        "elastic_net_model.fit(features_train, labels_train)\n",
        "\n",
        "# Predict and print results\n",
        "predicted_labels = elastic_net_model.predict(features_test)\n",
        "print(\"Elastic Net Logistic Regression Accuracy:\", accuracy_score(labels_test, predicted_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VO2fIq0pGKxW",
        "outputId": "08972d5a-8c05-4c6e-b724-a46985a9036c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elastic Net Logistic Regression Accuracy: 0.9649122807017544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr'"
      ],
      "metadata": {
        "id": "p42K7XAyCv8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris_data = load_iris()\n",
        "feature_matrix = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\n",
        "target_classes = pd.Series(iris_data.target)\n",
        "\n",
        "# Split dataset\n",
        "features_train, features_test, targets_train, targets_test = train_test_split(\n",
        "    feature_matrix, target_classes, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression model with One-vs-Rest classification\n",
        "ovr_logistic_model = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
        "ovr_logistic_model.fit(features_train, targets_train)\n",
        "\n",
        "# Predict and print accuracy\n",
        "predicted_targets = ovr_logistic_model.predict(features_test)\n",
        "print(\"One-vs-Rest Logistic Regression Accuracy:\", accuracy_score(targets_test, predicted_targets))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOoE0vFDGLCd",
        "outputId": "7741ad94-0328-4f7c-e069-95e64a0b922b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-Rest Logistic Regression Accuracy: 0.9666666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracyC"
      ],
      "metadata": {
        "id": "G6vF0abBCv4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']}\n",
        "model = GridSearchCV(LogisticRegression(solver='saga', max_iter=5000), param_grid, cv=5)\n",
        "\n",
        "# Fit and predict\n",
        "model.fit(X_train, y_train)\n",
        "best_model = model.best_estimator_\n",
        "predictions = best_model.predict(X_test)\n",
        "\n",
        "# Output\n",
        "print(\"Best Parameters:\", model.best_params_)\n",
        "print(\"Best Model Accuracy:\", accuracy_score(y_test, predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RShsCEmlGLan",
        "outputId": "7db99bd9-8bd3-4249-9996-2082534f9cee"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 0.1, 'penalty': 'l2'}\n",
            "Best Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracyC"
      ],
      "metadata": {
        "id": "Tlm_2k5-Cv0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_breast_cancer()\n",
        "features = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "labels = pd.Series(dataset.target)\n",
        "\n",
        "# Set up Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "logreg_model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Run cross-validation\n",
        "cv_scores = cross_val_score(logreg_model, features, labels, cv=skf, scoring='accuracy')\n",
        "print(\"Average Accuracy from 5-Fold CV:\", cv_scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_XXpVRfGLwW",
        "outputId": "04d3a1a9-e417-46e7-b439-e9fddbfa87d7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy from 5-Fold CV: 0.9473063188945815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy."
      ],
      "metadata": {
        "id": "rESDoHUsCvwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data from CSV file (assuming it has feature columns and a 'label' column)\n",
        "data_frame = pd.read_csv(\"/content/sample_data/california_housing_test.csv\")\n",
        "\n",
        "# Separate input variables and target variable\n",
        "features = data_frame.drop(columns=['housing_median_age'])\n",
        "target = data_frame['median_house_value']\n",
        "\n",
        "# Divide data into training and testing sets\n",
        "features_train, features_test, target_train, target_test = train_test_split(\n",
        "    features, target, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize and train logistic regression model\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "logreg.fit(features_train, target_train)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predicted_labels = logreg.predict(features_test)\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(target_test, predicted_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA8KYWPlGMQn",
        "outputId": "72f7af5e-a6c9-4ab6-86bb-f429e9f61f38"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracyM"
      ],
      "metadata": {
        "id": "VQTzBRkhCvr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_breast_cancer()\n",
        "features = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "target = pd.Series(dataset.target)\n",
        "\n",
        "# Split dataset\n",
        "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define hyperparameter search space\n",
        "hyperparameters = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# Apply RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(LogisticRegression(max_iter=1000), hyperparameters, cv=5, n_iter=10, random_state=42)\n",
        "random_search.fit(features_train, target_train)\n",
        "\n",
        "# Predict and print results\n",
        "best_logistic_model = random_search.best_estimator_\n",
        "predictions = best_logistic_model.predict(features_test)\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best Model Accuracy:\", accuracy_score(target_test, predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOWZPoOZGMoe",
        "outputId": "e26b0060-67f7-4e81-8506-039b4f1c2c3b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 100}\n",
            "Best Model Accuracy: 0.9824561403508771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy"
      ],
      "metadata": {
        "id": "-vYZwua9Cvn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris_data = load_iris()\n",
        "features_df = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\n",
        "target_series = pd.Series(iris_data.target)\n",
        "\n",
        "# Split dataset\n",
        "features_train, features_test, target_train, target_test = train_test_split(\n",
        "    features_df, target_series, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression with One-vs-Rest strategy\n",
        "logistic_model = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
        "logistic_model.fit(features_train, target_train)\n",
        "\n",
        "# Predict and print accuracy\n",
        "test_predictions = logistic_model.predict(features_test)\n",
        "print(\"One-vs-Rest Logistic Regression Accuracy:\", accuracy_score(target_test, test_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2dbwwEMGNKX",
        "outputId": "6f5856e4-b362-459b-a609-19fc88e17636"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-Rest Logistic Regression Accuracy: 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classification"
      ],
      "metadata": {
        "id": "YozmmbPNCvkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset\n",
        "cancer_dataset = load_breast_cancer()\n",
        "features_df = pd.DataFrame(cancer_dataset.data, columns=cancer_dataset.feature_names)\n",
        "target_series = pd.Series(cancer_dataset.target)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features_df, target_series, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression model\n",
        "log_reg_model = LogisticRegression(max_iter=1000)\n",
        "log_reg_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and compute confusion matrix\n",
        "y_predicted = log_reg_model.predict(X_test)\n",
        "conf_matrix = confusion_matrix(y_test, y_predicted)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "M_zUL3AHGNb-",
        "outputId": "845f108d-d0d8-411f-fc1c-e1d291cde6fc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x400 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbMAAAGJCAYAAAAADN1MAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAANpVJREFUeJzt3XlYVGXfB/DvGYQBwRkWkeVRFlMRc19SRMUFJUvToMc9ccvsQVNHrahMJAuzFHctH7dcnsxKS61MccvEjcQljVAxLBlcAUEZEM77h5fzNoI6AzMMc87303WuS+5zzn3/Dq+Pv/d3n3vuEURRFEFERGTDFNYOgIiIqLKYzIiIyOYxmRERkc1jMiMiIpvHZEZERDaPyYyIiGwekxkREdk8JjMiIrJ5TGZERGTzmMzIpqSnp6NXr15Qq9UQBAFbt241a/+XLl2CIAhYs2aNWfu1ZV27dkXXrl2tHQbRYzGZkckuXLiAV199FfXr14ejoyNUKhVCQ0OxYMEC3L1716JjR0dH4/Tp0/jggw+wbt06tG3b1qLjVaURI0ZAEASoVKpyf4/p6ekQBAGCIOCTTz4xuf8rV64gLi4OqampZoiWqHqpYe0AyLbs2LED//73v6FUKjF8+HA0bdoURUVFOHjwIKZNm4bffvsNn332mUXGvnv3LpKTk/HOO+9g/PjxFhnD398fd+/ehb29vUX6f5IaNWrgzp072LZtGwYMGGBwbsOGDXB0dERhYWGF+r5y5QpmzpyJgIAAtGzZ0uj7fvrppwqNR1SVmMzIaBkZGRg0aBD8/f2xZ88e+Pj46M/FxMTg/Pnz2LFjh8XGv3btGgDA1dXVYmMIggBHR0eL9f8kSqUSoaGh+N///lcmmW3cuBHPP/88vv766yqJ5c6dO6hZsyYcHByqZDyiyuA0Ixltzpw5yM/Px8qVKw0S2QMNGjTAxIkT9T/fu3cP77//Pp566ikolUoEBATg7bffhk6nM7gvICAAffr0wcGDB/HMM8/A0dER9evXx+eff66/Ji4uDv7+/gCAadOmQRAEBAQEALg/Pffgz/8UFxcHQRAM2nbt2oVOnTrB1dUVLi4uCAoKwttvv60//6h3Znv27EHnzp3h7OwMV1dX9OvXD+fOnSt3vPPnz2PEiBFwdXWFWq3GyJEjcefOnUf/Yh8yZMgQ/PDDD8jJydG3HTt2DOnp6RgyZEiZ62/evImpU6eiWbNmcHFxgUqlQu/evXHy5En9Nfv27UO7du0AACNHjtRPVz54zq5du6Jp06ZISUlBly5dULNmTf3v5eF3ZtHR0XB0dCzz/BEREXBzc8OVK1eMflYic2EyI6Nt27YN9evXR8eOHY26fsyYMXjvvffQunVrJCYmIiwsDAkJCRg0aFCZa8+fP4+XXnoJPXv2xNy5c+Hm5oYRI0bgt99+AwBERkYiMTERADB48GCsW7cO8+fPNyn+3377DX369IFOp0N8fDzmzp2LF154Ab/88stj79u9ezciIiJw9epVxMXFQaPR4NChQwgNDcWlS5fKXD9gwADcvn0bCQkJGDBgANasWYOZM2caHWdkZCQEQcA333yjb9u4cSMaN26M1q1bl7n+4sWL2Lp1K/r06YN58+Zh2rRpOH36NMLCwvSJJTg4GPHx8QCAsWPHYt26dVi3bh26dOmi7+fGjRvo3bs3WrZsifnz56Nbt27lxrdgwQJ4enoiOjoaJSUlAIBPP/0UP/30ExYtWgRfX1+jn5XIbEQiI+Tm5ooAxH79+hl1fWpqqghAHDNmjEH71KlTRQDinj179G3+/v4iAPHAgQP6tqtXr4pKpVKcMmWKvi0jI0MEIH788ccGfUZHR4v+/v5lYpgxY4b4z7/iiYmJIgDx2rVrj4z7wRirV6/Wt7Vs2VKsU6eOeOPGDX3byZMnRYVCIQ4fPrzMeKNGjTLo88UXXxQ9PDweOeY/n8PZ2VkURVF86aWXxB49eoiiKIolJSWit7e3OHPmzHJ/B4WFhWJJSUmZ51AqlWJ8fLy+7dixY2We7YGwsDARgLh8+fJyz4WFhRm07dy5UwQgzpo1S7x48aLo4uIi9u/f/4nPSGQprMzIKHl5eQCAWrVqGXX9999/DwDQaDQG7VOmTAGAMu/WmjRpgs6dO+t/9vT0RFBQEC5evFjhmB/24F3bt99+i9LSUqPuycrKQmpqKkaMGAF3d3d9e/PmzdGzZ0/9c/7TuHHjDH7u3Lkzbty4of8dGmPIkCHYt28ftFot9uzZA61WW+4UI3D/PZtCcf9/yiUlJbhx44Z+CvXXX381ekylUomRI0cadW2vXr3w6quvIj4+HpGRkXB0dMSnn35q9FhE5sZkRkZRqVQAgNu3bxt1/Z9//gmFQoEGDRoYtHt7e8PV1RV//vmnQbufn1+ZPtzc3HDr1q0KRlzWwIEDERoaijFjxsDLywuDBg3Cl19++djE9iDOoKCgMueCg4Nx/fp1FBQUGLQ//Cxubm4AYNKzPPfcc6hVqxY2bdqEDRs2oF27dmV+lw+UlpYiMTERDRs2hFKpRO3ateHp6YlTp04hNzfX6DH/9a9/mbTY45NPPoG7uztSU1OxcOFC1KlTx+h7icyNyYyMolKp4OvrizNnzph038MLMB7Fzs6u3HZRFCs8xoP3OQ84OTnhwIED2L17N15++WWcOnUKAwcORM+ePctcWxmVeZYHlEolIiMjsXbtWmzZsuWRVRkAfPjhh9BoNOjSpQvWr1+PnTt3YteuXXj66aeNrkCB+78fU5w4cQJXr14FAJw+fdqke4nMjcmMjNanTx9cuHABycnJT7zW398fpaWlSE9PN2jPzs5GTk6OfmWiObi5uRms/Hvg4eoPABQKBXr06IF58+bh7Nmz+OCDD7Bnzx7s3bu33L4fxJmWllbm3O+//47atWvD2dm5cg/wCEOGDMGJEydw+/btchfNPPDVV1+hW7duWLlyJQYNGoRevXohPDy8zO/E2P/HwhgFBQUYOXIkmjRpgrFjx2LOnDk4duyY2fonMhWTGRntjTfegLOzM8aMGYPs7Owy5y9cuIAFCxYAuD9NBqDMisN58+YBAJ5//nmzxfXUU08hNzcXp06d0rdlZWVhy5YtBtfdvHmzzL0PPjz88McFHvDx8UHLli2xdu1ag+Rw5swZ/PTTT/rntIRu3brh/fffx+LFi+Ht7f3I6+zs7MpUfZs3b8bff/9t0PYg6ZaX+E315ptvIjMzE2vXrsW8efMQEBCA6OjoR/4eiSyNH5omoz311FPYuHEjBg4ciODgYIMdQA4dOoTNmzdjxIgRAIAWLVogOjoan332GXJychAWFoajR49i7dq16N+//yOXfVfEoEGD8Oabb+LFF1/E66+/jjt37mDZsmVo1KiRwQKI+Ph4HDhwAM8//zz8/f1x9epVLF26FHXr1kWnTp0e2f/HH3+M3r17IyQkBKNHj8bdu3exaNEiqNVqxMXFme05HqZQKPDuu+8+8bo+ffogPj4eI0eORMeOHXH69Gls2LAB9evXN7juqaeegqurK5YvX45atWrB2dkZ7du3R2BgoElx7dmzB0uXLsWMGTP0HxVYvXo1unbtiunTp2POnDkm9UdkFlZeTUk26I8//hBfeeUVMSAgQHRwcBBr1aolhoaGiosWLRILCwv11xUXF4szZ84UAwMDRXt7e7FevXpibGyswTWieH9p/vPPP19mnIeXhD9qab4oiuJPP/0kNm3aVHRwcBCDgoLE9evXl1man5SUJPbr10/09fUVHRwcRF9fX3Hw4MHiH3/8UWaMh5ev7969WwwNDRWdnJxElUol9u3bVzx79qzBNQ/Ge3jp/+rVq0UAYkZGxiN/p6JouDT/UR61NH/KlCmij4+P6OTkJIaGhorJycnlLqn/9ttvxSZNmog1atQweM6wsDDx6aefLnfMf/aTl5cn+vv7i61btxaLi4sNrps8ebKoUCjE5OTkxz4DkSUIomjCW2kiIqJqiO/MiIjI5jGZERGRzWMyIyIim8dkRkREFhMQEKD/loZ/HjExMQCAwsJCxMTEwMPDAy4uLoiKiir3oz9PwgUgRERkMdeuXTPYYefMmTPo2bMn9u7di65du+K1117Djh07sGbNGqjVaowfPx4KheKJ32bxMCYzIiKqMpMmTcL27duRnp6OvLw8eHp6YuPGjXjppZcA3N9ZJzg4GMnJyejQoYPR/XKakYiITKLT6ZCXl2dwGLP7S1FREdavX49Ro0ZBEASkpKSguLgY4eHh+msaN24MPz8/o7bN+ydJ7gAybP3JJ19EZAaLIptaOwSSCbea5W9gXVFOrcZX+N43+9Uu84WzM2bMeOKOOFu3bkVOTo5+pyCtVgsHBwf91zM94OXlBa1Wa1JMkkxmRET0BELFJ+ZiY2PLfFehUql84n0rV65E7969LfJt5ExmRERyVIlvUVAqlUYlr3/6888/sXv3bnzzzTf6Nm9vbxQVFSEnJ8egOsvOzn7s5trl4TszIiI5EhQVPypg9erVqFOnjsE3ZrRp0wb29vZISkrSt6WlpSEzMxMhISEm9c/KjIiILKq0tBSrV69GdHQ0atT4/7SjVqsxevRoaDQauLu7Q6VSYcKECQgJCTFpJSPAZEZEJE9m/LLWJ9m9ezcyMzMxatSoMucSExOhUCgQFRUFnU6HiIgILF261OQxJPk5M65mpKrC1YxUVcy+mvGZqRW+9+7RT8wYiXmwMiMikqMqrMyqApMZEZEcVWJpfnXEZEZEJEcSq8yklZqJiEiWWJkREckRpxmJiMjmSWyakcmMiEiOWJkREZHNY2VGREQ2T2KVmbSehoiIZImVGRGRHEmsMmMyIyKSIwXfmRERka1jZUZERDaPqxmJiMjmSawyk9bTEBGRLLEyIyKSI04zEhGRzZPYNCOTGRGRHLEyIyIim8fKjIiIbJ7EKjNppWYiIpIlVmZERHLEaUYiIrJ5EptmZDIjIpIjVmZERGTzmMyIiMjmSWyaUVqpmYiIZImVGRGRHHGakYiIbJ7EphmZzIiI5IiVGRER2TxWZkREZOsEiSUzadWZRERU7fz9998YNmwYPDw84OTkhGbNmuH48eP686Io4r333oOPjw+cnJwQHh6O9PR0k8ZgMiMikiFBECp8mOLWrVsIDQ2Fvb09fvjhB5w9exZz586Fm5ub/po5c+Zg4cKFWL58OY4cOQJnZ2dERESgsLDQ6HE4zUhEJEdVNMv40UcfoV69eli9erW+LTAwUP9nURQxf/58vPvuu+jXrx8A4PPPP4eXlxe2bt2KQYMGGTUOKzMiIhmqTGWm0+mQl5dncOh0unLH+e6779C2bVv8+9//Rp06ddCqVSusWLFCfz4jIwNarRbh4eH6NrVajfbt2yM5Odno52EyIyKSocoks4SEBKjVaoMjISGh3HEuXryIZcuWoWHDhti5cydee+01vP7661i7di0AQKvVAgC8vLwM7vPy8tKfMwanGYmIZKgyqxljY2Oh0WgM2pRKZbnXlpaWom3btvjwww8BAK1atcKZM2ewfPlyREdHVziGh7EyIyIikyiVSqhUKoPjUcnMx8cHTZo0MWgLDg5GZmYmAMDb2xsAkJ2dbXBNdna2/pwxmMyIiGSoqlYzhoaGIi0tzaDtjz/+gL+/P4D7i0G8vb2RlJSkP5+Xl4cjR44gJCTE6HE4zUhEJEdVtJpx8uTJ6NixIz788EMMGDAAR48exWeffYbPPvvsfhiCgEmTJmHWrFlo2LAhAgMDMX36dPj6+qJ///5Gj8NkRkQkQ1W1A0i7du2wZcsWxMbGIj4+HoGBgZg/fz6GDh2qv+aNN95AQUEBxo4di5ycHHTq1Ak//vgjHB0djR5HEEVRtMQDWNOw9SetHQLJxKLIptYOgWTCraadefsbtqHC995aP/TJF1UxVmZERDLEvRmJiIiqGVZmREQyJLXKjMmMiEiOpJXLmMyIiOSIlRkREdk8JjMiIrJ5UktmXM1IREQ2j5UZEZEcSaswYzIjIpIjqU0zMpkREckQkxkREdk8JjMiIrJ5UktmXM1IREQ2j5UZEZEcSaswYzIjIpIjqU0zMpkREckQkxkREdk8qSUzLgAhIiKbx8qMiEiOpFWYMZnJSY+GHujRyAOezg4AgL9yC7HldDZOXbkNAKjj4oAhrX3RqI4z7BUCTmXdxtpjfyOv8J41wyYJ+nzVCixdlIiBQ17G5Gmx1g5HlqQ2zchkJiM37xRj04ksaG/rIADoXN8dmrAAvPP9H7ieX4w3e9RH5q27+HD3BQDASy28MaVrIOJ+TIdo3dBJQs7+dhpbvv4SDRoGWTsUWZNaMuM7Mxk58XceTl65jezbRdDeLsLmk1oU3itFg9rOaFinJjydHfBZ8mX8lVOIv3IK8emhTAR6OKGJt4u1QyeJuHOnADPefgOx02eilkpl7XBkTRCECh/VEZOZTAkC0MHfFcoaCqRfL4C9QgERQHHJ/9dgxSUiRBEIquNsvUBJUj5JmIXQzmF4pkNHa4cie1JLZladZrx+/TpWrVqF5ORkaLVaAIC3tzc6duyIESNGwNPT05rhSVJdV0fERTSAvZ0ChfdKMX//JVzJ1eF24T3o7pViUCsffJmaBQECBrbygZ1CgKuTvbXDJgnY9eP3SPv9LFat/9LaoZAEWS2ZHTt2DBEREahZsybCw8PRqFEjAEB2djYWLlyI2bNnY+fOnWjbtu1j+9HpdNDpdAZtJcVFsLN3sFjstiwrT4d3dvwBJwc7POOnxqsd/TBr13lcydVh4c+XMPKZuujVuDZEEUi+dAsZN+6gVOQbM6qcbG0W5n2cgIXL/gulUmntcAiQ3GpGQRSt8y9Vhw4d0KJFCyxfvrxM2SqKIsaNG4dTp04hOTn5sf3ExcVh5syZBm3NXnwVzSNfM3vMUvRWj/q4ml+EVUf+0re5KO1QWiriTnEpFkc1wQ/nrmHH2WtWjLL6WhTZ1Noh2IT9e3fjTc3rsLOz07eVlJRAEAQoFAocOJJqcI7Kcqtp3t9Pfc33Fb734rznzBiJeVitMjt58iTWrFlT7vyrIAiYPHkyWrVq9cR+YmNjodFoDNpe/TrNbHFKnSAANRSG/zfI15UAAJp4uUDlWAO//pVnjdBIQto+E4INm781aJs14x34Bwbi5RFjmMisoLq++6ooqyUzb29vHD16FI0bNy73/NGjR+Hl5fXEfpRKZZlpC04xlm9AS2+cvHIbNwqK4Ghvh44Brgj2csGcpIsAgC713fB33v33Zw09a2JY23/hx3PXkJWne0LPRI/n7OyMpxo0NGhzdHKCWu1app2qhsRymfWS2dSpUzF27FikpKSgR48e+sSVnZ2NpKQkrFixAp988om1wpMklWMNjOvoB1enGrhTXILLtwoxJ+kizmjzAQA+KkcMaOUDFwc7XCsoxndnsvHDuetWjpqILEFqlZnV3pkBwKZNm5CYmIiUlBSUlNyf2rKzs0ObNm2g0WgwYMCACvU7bP1Jc4ZJ9Eh8Z0ZVxdzvzBpO+7HC96Z//KwZIzEPqy7NHzhwIAYOHIji4mJcv36/Aqhduzbs7bkUnIjIkiRWmFWP7azs7e3h4+Nj7TCIiGRDatOM1SKZERFR1ZJYLuN2VkREcqRQCBU+TBEXF1dmO6x/rmIvLCxETEwMPDw84OLigqioKGRnZ5v+PCbfQURENk8QKn6Y6umnn0ZWVpb+OHjwoP7c5MmTsW3bNmzevBn79+/HlStXEBkZafIYnGYkIiKLqlGjBry9vcu05+bmYuXKldi4cSO6d+8OAFi9ejWCg4Nx+PBhdOjQwegxWJkREclQZXbN1+l0yMvLMzge3iP3n9LT0+Hr64v69etj6NChyMzMBACkpKSguLgY4eHh+msbN24MPz+/J25l+DAmMyIiGarMNGNCQgLUarXBkZCQUO447du3x5o1a/Djjz9i2bJlyMjIQOfOnXH79m1otVo4ODjA1dXV4B4vLy/9N6kYi9OMREQyVJml+eXtifuob0Po3bu3/s/NmzdH+/bt4e/vjy+//BJOTk4VjuFhTGZERDJUmWRW3p64xnJ1dUWjRo1w/vx59OzZE0VFRcjJyTGozrKzs8t9x/Y4nGYkIpKhqlzN+E/5+fm4cOECfHx80KZNG9jb2yMpKUl/Pi0tDZmZmQgJCTGpX1ZmRERkMVOnTkXfvn3h7++PK1euYMaMGbCzs8PgwYOhVqsxevRoaDQauLu7Q6VSYcKECQgJCTFpJSPAZEZEJEtVtZ3VX3/9hcGDB+PGjRvw9PREp06dcPjwYXh6egIAEhMToVAoEBUVBZ1Oh4iICCxdutTkcZjMiIhkqKq2s/riiy8ee97R0RFLlizBkiVLKjUOkxkRkQxxo2EiIrJ5EstlTGZERHIktcqMS/OJiMjmsTIjIpIhiRVmTGZERHIktWlGJjMiIhmSWC5jMiMikiNWZkREZPMklsu4mpGIiGwfKzMiIhniNCMREdk8ieUyJjMiIjliZUZERDaPyYyIiGyexHIZVzMSEZHtY2VGRCRDnGYkIiKbJ7FcxmRGRCRHrMyIiMjmSSyXMZkREcmRQmLZjKsZiYjI5rEyIyKSIYkVZkxmRERyJMsFIKdOnTK6w+bNm1c4GCIiqhoKaeUy45JZy5YtIQgCRFEs9/yDc4IgoKSkxKwBEhGR+cmyMsvIyLB0HEREVIUklsuMS2b+/v6WjoOIiKjCKrQ0f926dQgNDYWvry/+/PNPAMD8+fPx7bffmjU4IiKyDKES/1VHJiezZcuWQaPR4LnnnkNOTo7+HZmrqyvmz59v7viIiMgCFELFj+rI5GS2aNEirFixAu+88w7s7Oz07W3btsXp06fNGhwREVmGIAgVPqojkz9nlpGRgVatWpVpVyqVKCgoMEtQRERkWdU0J1WYyZVZYGAgUlNTy7T/+OOPCA4ONkdMRERkYQpBqPBRHZlcmWk0GsTExKCwsBCiKOLo0aP43//+h4SEBPz3v/+1RIxERESPZXJlNmbMGHz00Ud49913cefOHQwZMgTLli3DggULMGjQIEvESEREZiYIFT8qavbs2RAEAZMmTdK3FRYWIiYmBh4eHnBxcUFUVBSys7NN7rtCS/OHDh2K9PR05OfnQ6vV4q+//sLo0aMr0hUREVlBVS8AOXbsGD799NMyWx5OnjwZ27Ztw+bNm7F//35cuXIFkZGRJvdf4a+AuXr1KlJSUpCWloZr165VtBsiIrKCqqzM8vPzMXToUKxYsQJubm769tzcXKxcuRLz5s1D9+7d0aZNG6xevRqHDh3C4cOHTRrD5GR2+/ZtvPzyy/D19UVYWBjCwsLg6+uLYcOGITc319TuiIjICiqzAESn0yEvL8/g0Ol0jxwrJiYGzz//PMLDww3aU1JSUFxcbNDeuHFj+Pn5ITk52bTnMe3x778zO3LkCHbs2IGcnBzk5ORg+/btOH78OF599VVTuyMiIisQKnEkJCRArVYbHAkJCeWO88UXX+DXX38t97xWq4WDgwNcXV0N2r28vKDVak16HpNXM27fvh07d+5Ep06d9G0RERFYsWIFnn32WVO7IyIiGxMbGwuNRmPQplQqy1x3+fJlTJw4Ebt27YKjo6NFYzI5mXl4eECtVpdpV6vVBnOhRERUfVVmJw+lUllu8npYSkoKrl69itatW+vbSkpKcODAASxevBg7d+5EUVERcnJyDKqz7OxseHt7mxSTydOM7777LjQajUEJqNVqMW3aNEyfPt3U7oiIyAqqYm/GHj164PTp00hNTdUfbdu2xdChQ/V/tre3R1JSkv6etLQ0ZGZmIiQkxKTnMaoya9WqlUEWT09Ph5+fH/z8/AAAmZmZUCqVuHbtGt+bERHZgKrYY7FWrVpo2rSpQZuzszM8PDz07aNHj4ZGo4G7uztUKhUmTJiAkJAQdOjQwaSxjEpm/fv3N6lTIiKq3qrLrlSJiYlQKBSIioqCTqdDREQEli5danI/giiKogXis6ph609aOwSSiUWRTZ98EZEZuNW0e/JFJhi+8VSF7/18SPMnX1TFKvyhaSIiourC5NWMJSUlSExMxJdffonMzEwUFRUZnL9586bZgiMiIsuorl+yWVEmV2YzZ87EvHnzMHDgQOTm5kKj0SAyMhIKhQJxcXEWCJGIiMxNal/OaXIy27BhA1asWIEpU6agRo0aGDx4MP773//ivffeM3kvLSIiso7K7ABSHZmczLRaLZo1awYAcHFx0e/H2KdPH+zYscO80RERkUVI7cs5TU5mdevWRVZWFgDgqaeewk8//QTg/vb+xnwinIiIyNxMTmYvvvii/tPaEyZMwPTp09GwYUMMHz4co0aNMnuARERkftb4ck5LMnk14+zZs/V/HjhwIPz9/XHo0CE0bNgQffv2NWtwRERkGdV1IUdFVfpzZh06dIBGo0H79u3x4YcfmiMmIiKyMKlVZmb70HRWVhY3GiYishFSWwBi8jQjERHZvmqakyqM21kREZHNY2VGRCRDUlsAYnQye/grsh927dq1SgdjLv8d1MLaIZBMuLUbb+0QSCbunlhs1v6kNi1ndDI7ceLEE6/p0qVLpYIhIqKqIdvKbO/evZaMg4iIqpDUds3nOzMiIhmSWjKT2rQpERHJECszIiIZku07MyIikg6pTTMymRERyZDECrOKvTP7+eefMWzYMISEhODvv/8GAKxbtw4HDx40a3BERGQZUtub0eRk9vXXXyMiIgJOTk44ceIEdDodACA3N5e75hMR2QhFJY7qyOS4Zs2aheXLl2PFihWwt7fXt4eGhuLXX381a3BERETGMPmdWVpaWrk7fajVauTk5JgjJiIisrBqOltYYSZXZt7e3jh//nyZ9oMHD6J+/fpmCYqIiCxL9u/MXnnlFUycOBFHjhyBIAi4cuUKNmzYgKlTp+K1116zRIxERGRmUvumaZOnGd966y2UlpaiR48euHPnDrp06QKlUompU6diwoQJloiRiIjMTPafMxMEAe+88w6mTZuG8+fPIz8/H02aNIGLi4sl4iMiIguortOFFVXhD007ODigSZMm5oyFiIioQkxOZt26dXvsnl579uypVEBERGR5EivMTE9mLVu2NPi5uLgYqampOHPmDKKjo80VFxERWZDs35klJiaW2x4XF4f8/PxKB0RERJYnQFrZzGw7kwwbNgyrVq0yV3dERGRBCqHihymWLVuG5s2bQ6VSQaVSISQkBD/88IP+fGFhIWJiYuDh4QEXFxdERUUhOzvb9Ocx+Y5HSE5OhqOjo7m6IyIiC6qqZFa3bl3Mnj0bKSkpOH78OLp3745+/frht99+AwBMnjwZ27Ztw+bNm7F//35cuXIFkZGRJj+PydOMDw8iiiKysrJw/PhxTJ8+3eQAiIhIuvr27Wvw8wcffIBly5bh8OHDqFu3LlauXImNGzeie/fuAIDVq1cjODgYhw8fRocOHYwex+RkplarDX5WKBQICgpCfHw8evXqZWp3RERkBZX5pmmdTqf/xpQHlEollErlY+8rKSnB5s2bUVBQgJCQEKSkpKC4uBjh4eH6axo3bgw/Pz8kJydbLpmVlJRg5MiRaNasGdzc3Ey5lYiIqpHKrGZMSEjAzJkzDdpmzJiBuLi4cq8/ffo0QkJCUFhYCBcXF2zZsgVNmjRBamoqHBwc4OrqanC9l5cXtFqtSTGZlMzs7OzQq1cvnDt3jsmMiMiGVeZzZrGxsdBoNAZtj6vKgoKCkJqaitzcXHz11VeIjo7G/v37Kx5AOUyeZmzatCkuXryIwMBAswZCRERVpzLbWRkzpfhPDg4OaNCgAQCgTZs2OHbsGBYsWICBAweiqKgIOTk5BtVZdnY2vL29TYqpQl/OOXXqVGzfvh1ZWVnIy8szOIiIqPqrqtWM5SktLYVOp0ObNm1gb2+PpKQk/bm0tDRkZmYiJCTEpD6Nrszi4+MxZcoUPPfccwCAF154weAFoiiKEAQBJSUlJgVARETSFRsbi969e8PPzw+3b9/Gxo0bsW/fPuzcuRNqtRqjR4+GRqOBu7s7VCoVJkyYgJCQEJMWfwAmJLOZM2di3Lhx2Lt3r8kPQ0RE1UtV7c149epVDB8+HFlZWVCr1WjevDl27tyJnj17Ari/q5RCoUBUVBR0Oh0iIiKwdOlSk8cRRFEUjblQoVBAq9WiTp06Jg9S1QrvWTsCkgu3duOtHQLJxN0Ti83a35JfLlX43pjQALPFYS4mLQCpzOcSiIio+pDaP+cmJbNGjRo9MaHdvHmzUgEREZHlyXrX/JkzZ5bZAYSIiGyPrL9petCgQTbxzoyIiOTF6GTG92VERNIhtX/SjU5mRi56JCIiGyDbacbS0lJLxkFERFVIYrnM9L0ZiYjI9pntm5mrCSYzIiIZkto6CKklZyIikiFWZkREMiStuozJjIhIlmS7mpGIiKRDWqmMyYyISJYkVpgxmRERyRFXMxIREVUzrMyIiGRIapUMkxkRkQxJbZqRyYyISIaklcqYzIiIZImVGRER2TypvTOT2vMQEZEMsTIjIpIhTjMSEZHNk1YqYzIjIpIliRVmTGZERHKkkFhtxmRGRCRDUqvMuJqRiIhsHiszIiIZEjjNSEREtk5q04xMZkREMsQFIEREZPNYmRERkc2TWjLjakYiIrJ5TGZERDIkVOI/UyQkJKBdu3aoVasW6tSpg/79+yMtLc3gmsLCQsTExMDDwwMuLi6IiopCdna2SeMwmRERyZBCqPhhiv379yMmJgaHDx/Grl27UFxcjF69eqGgoEB/zeTJk7Ft2zZs3rwZ+/fvx5UrVxAZGWnSOIIoiqJpoVV/hfesHQHJhVu78dYOgWTi7onFZu1vz+83Knxv98YeFb732rVrqFOnDvbv348uXbogNzcXnp6e2LhxI1566SUAwO+//47g4GAkJyejQ4cORvXLyoyISIYEoeKHTqdDXl6ewaHT6YwaNzc3FwDg7u4OAEhJSUFxcTHCw8P11zRu3Bh+fn5ITk42+nmYzIiIyCQJCQlQq9UGR0JCwhPvKy0txaRJkxAaGoqmTZsCALRaLRwcHODq6mpwrZeXF7RardExcWk+EZEMVWY7q9jYWGg0GoM2pVL5xPtiYmJw5swZHDx4sMJjPwqTmcylHD+GNatW4tzZM7h27RoSFy5B9x7hT76R6DF+3zET/r5l36ss33QAk2d/CaVDDczWROLfEW2gdKiB3cnnMPHDTbh687YVopUnUxdy/JNSqTQqef3T+PHjsX37dhw4cAB169bVt3t7e6OoqAg5OTkG1Vl2dja8vb2N7p/TjDJ39+4dBAUFIfbdGdYOhSSk07CPERAeqz+eG7cIAPDNrhMAgDlTo/B8l6YY+sZK9BozHz6eanwxd4w1Q5adqlqaL4oixo8fjy1btmDPnj0IDAw0ON+mTRvY29sjKSlJ35aWlobMzEyEhIQYPQ4rM5nr1DkMnTqHWTsMkpjrt/INfp46sikuZF7DzynpULk4YkT/EIx4ew32H/sDADB2xnqc3DIdzzQLwNHTl6wQsfxU1Q4gMTEx2LhxI7799lvUqlVL/x5MrVbDyckJarUao0ePhkajgbu7O1QqFSZMmICQkBCjVzICrMyIyMLsa9hh0HPtsPbb+yvTWgX7wcG+BvYc/v8Pzv5xKRuZWTfRvnngo7ohMxMqcZhi2bJlyM3NRdeuXeHj46M/Nm3apL8mMTERffr0QVRUFLp06QJvb2988803Jo3DyoyILOqFbs3hWssJ67cdAQB4e6igKypGbv5dg+uu3siDl4fKGiGSBRnzUWZHR0csWbIES5YsqfA41boyu3z5MkaNGvXYayrzeQcisrzo/h2x85ezyLqWa+1Q6B8UglDhozqq1sns5s2bWLt27WOvKe/zDh9/9OTPOxCR5fn5uKF7+yCs2XpI36a9kQelgz3ULk4G19bxUCH7Rl5VhyhbVTXNWFWsOs343XffPfb8xYsXn9hHeZ93EO1MWzJKRJbx8gshuHrzNn74+Td924lzmSgqvodu7YOwNSkVANDQvw78fNxx5FSGlSKVoeqalSrIqsmsf//+EAThsXOqwhNK2vI+78C9GY13p6AAmZmZ+p///usv/H7uHNRqNXx8fa0YGdk6QRAwvF8HbNh+BCUlpfr2vPxCrNmajI+mROJmbgFuFxRi3pv/xuGTF7mSsQpV5kPT1ZFVk5mPjw+WLl2Kfv36lXs+NTUVbdq0qeKo5OW3385gzMjh+p8/mXN/ivaFfi/i/Q9nWysskoDu7YPg5+OOtVsPlzn3xidfo7RUxP8+GXP/Q9OHzmFiwqZyeiFLqaavvirMqrvmv/DCC2jZsiXi4+PLPX/y5Em0atUKpaWl5Z5/FFZmVFW4az5VFXPvmn/0YsUX5DxTX23GSMzDqpXZtGnTDL7T5mENGjTA3r17qzAiIiJ5kFhhZt1k1rlz58eed3Z2RlgYd6cgIjI7iWUzfmiaiEiGuACEiIhsntQWgDCZERHJkMRyWfXeAYSIiMgYrMyIiORIYqUZkxkRkQxxAQgREdk8LgAhIiKbJ7FcxmRGRCRLEstmXM1IREQ2j5UZEZEMcQEIERHZPC4AISIimyexXMZkRkQkSxLLZkxmREQyJLV3ZlzNSERENo+VGRGRDHEBCBER2TyJ5TImMyIiWZJYNmMyIyKSIaktAGEyIyKSIam9M+NqRiIisnmszIiIZEhihRmTGRGRLEksmzGZERHJEBeAEBGRzeMCECIisnlCJQ5THDhwAH379oWvry8EQcDWrVsNzouiiPfeew8+Pj5wcnJCeHg40tPTTX4eJjMiIrKYgoICtGjRAkuWLCn3/Jw5c7Bw4UIsX74cR44cgbOzMyIiIlBYWGjSOJxmJCKSoyqaZuzduzd69+5d7jlRFDF//ny8++676NevHwDg888/h5eXF7Zu3YpBgwYZPQ4rMyIiGRIq8Z9Op0NeXp7BodPpTI4hIyMDWq0W4eHh+ja1Wo327dsjOTnZpL6YzIiIZEgQKn4kJCRArVYbHAkJCSbHoNVqAQBeXl4G7V5eXvpzxuI0IxGRDFVmljE2NhYajcagTalUVi6gSmIyIyKSo0pkM6VSaZbk5e3tDQDIzs6Gj4+Pvj07OxstW7Y0qS9OMxIRkVUEBgbC29sbSUlJ+ra8vDwcOXIEISEhJvXFyoyISIaqageQ/Px8nD9/Xv9zRkYGUlNT4e7uDj8/P0yaNAmzZs1Cw4YNERgYiOnTp8PX1xf9+/c3aRwmMyIiGaqqHUCOHz+Obt266X9+8K4tOjoaa9aswRtvvIGCggKMHTsWOTk56NSpE3788Uc4OjqaNI4giqJo1sirgcJ71o6A5MKt3Xhrh0AycffEYrP2d/mm6UvpH6jnbt3FHuVhZUZEJENS25uRyYyISJaklc24mpGIiGweKzMiIhniNCMREdk8ieUyJjMiIjliZUZERDavqj40XVWYzIiI5EhauYyrGYmIyPaxMiMikiGJFWZMZkREcsQFIEREZPO4AISIiGyftHIZkxkRkRxJLJdxNSMREdk+VmZERDLEBSBERGTzuACEiIhsntQqM74zIyIim8fKjIhIhliZERERVTOszIiIZIgLQIiIyOZJbZqRyYyISIYklsuYzIiIZEli2YwLQIiIyOaxMiMikiEuACEiIpvHBSBERGTzJJbLmMyIiGRJYtmMyYyISIak9s6MqxmJiMjmsTIjIpIhqS0AEURRFK0dBFmfTqdDQkICYmNjoVQqrR0OSRj/rpElMJkRACAvLw9qtRq5ublQqVTWDockjH/XyBL4zoyIiGwekxkREdk8JjMiIrJ5TGYEAFAqlZgxYwZfyJPF8e8aWQIXgBARkc1jZUZERDaPyYyIiGwekxkREdk8JjMiIrJ5TGaEJUuWICAgAI6Ojmjfvj2OHj1q7ZBIgg4cOIC+ffvC19cXgiBg69at1g6JJITJTOY2bdoEjUaDGTNm4Ndff0WLFi0QERGBq1evWjs0kpiCggK0aNECS5YssXYoJEFcmi9z7du3R7t27bB48WIAQGlpKerVq4cJEybgrbfesnJ0JFWCIGDLli3o37+/tUMhiWBlJmNFRUVISUlBeHi4vk2hUCA8PBzJyclWjIyIyDRMZjJ2/fp1lJSUwMvLy6Ddy8sLWq3WSlEREZmOyYyIiGwek5mM1a5dG3Z2dsjOzjZoz87Ohre3t5WiIiIyHZOZjDk4OKBNmzZISkrSt5WWliIpKQkhISFWjIyIyDQ1rB0AWZdGo0F0dDTatm2LZ555BvPnz0dBQQFGjhxp7dBIYvLz83H+/Hn9zxkZGUhNTYW7uzv8/PysGBlJAZfmExYvXoyPP/4YWq0WLVu2xMKFC9G+fXtrh0USs2/fPnTr1q1Me3R0NNasWVP1AZGkMJkREZHN4zszIiKyeUxmRERk85jMiIjI5jGZERGRzWMyIyIim8dkRkRENo/JjIiIbB6TGRER2TwmM5KsESNGGHz5Y9euXTFp0qQqj2Pfvn0QBAE5OTkWG+PhZ62IqoiTyFKYzKhKjRgxAoIgQBAEODg4oEGDBoiPj8e9e/csPvY333yD999/36hrq/of9oCAAMyfP79KxiKSIm40TFXu2WefxerVq6HT6fD9998jJiYG9vb2iI2NLXNtUVERHBwczDKuu7u7WfohouqHlRlVOaVSCW9vb/j7++O1115DeHg4vvvuOwD/P132wQcfwNfXF0FBQQCAy5cvY8CAAXB1dYW7uzv69euHS5cu6fssKSmBRqOBq6srPDw88MYbb+DhbUcfnmbU6XR48803Ua9ePSiVSjRo0AArV67EpUuX9Bviurm5QRAEjBgxAsD9r8hJSEhAYGAgnJyc0KJFC3z11VcG43z//fdo1KgRnJyc0K1bN4M4K6KkpASjR4/WjxkUFIQFCxaUe+3MmTPh6ekJlUqFcePGoaioSH/OmNiJbBUrM7I6Jycn3LhxQ/9zUlISVCoVdu3aBQAoLi5GREQEQkJC8PPPP6NGjRqYNWsWnn32WZw6dQoODg6YO3cu1qxZg1WrViE4OBhz587Fli1b0L1790eOO3z4cCQnJ2PhwoVo0aIFMjIycP36ddSrVw9ff/01oqKikJaWBpVKBScnJwBAQkIC1q9fj+XLl6Nhw4Y4cOAAhg0bBk9PT4SFheHy5cuIjIxETEwMxo4di+PHj2PKlCmV+v2Ulpaibt262Lx5Mzw8PHDo0CGMHTsWPj4+GDBggMHvzdHREfv27cOlS5cwcuRIeHh44IMPPjAqdiKbJhJVoejoaLFfv36iKIpiaWmpuGvXLlGpVIpTp07Vn/fy8hJ1Op3+nnXr1olBQUFiaWmpvk2n04lOTk7izp07RVEURR8fH3HOnDn688XFxWLdunX1Y4miKIaFhYkTJ04URVEU09LSRADirl27yo1z7969IgDx1q1b+rbCwkKxZs2a4qFDhwyuHT16tDh48GBRFEUxNjZWbNKkicH5N998s0xfD/P39xcTExMfef5hMTExYlRUlP7n6Oho0d3dXSwoKNC3LVu2THRxcRFLSkqMir28ZyayFazMqMpt374dLi4uKC4uRmlpKYYMGYK4uDj9+WbNmhm8Jzt58iTOnz+PWrVqGfRTWFiICxcuIDc3F1lZWQbfwVajRg20bdu2zFTjA6mpqbCzszOpIjl//jzu3LmDnj17GrQXFRWhVatWAIBz586V+S44c3xr95IlS7Bq1SpkZmbi7t27KCoqQsuWLQ2uadGiBWrWrGkwbn5+Pi5fvoz8/Pwnxk5ky5jMqMp169YNy5Ytg4ODA3x9fVGjhuFfQ2dnZ4Of8/Pz0aZNG2zYsKFMX56enhWK4cG0oSny8/MBADt27MC//vUvg3NKpbJCcRjjiy++wNSpUzF37lyEhISgVq1a+Pjjj3HkyBGj+7BW7ERVhcmMqpyzszMaNGhg9PWtW7fGpk2bUKdOHahUqnKv8fHxwZEjR9ClSxcAwL1795CSkoLWrVuXe32zZs1QWlqK/fv3Izw8vMz5B5VhSUmJvq1JkyZQKpXIzMx8ZEUXHBysX8zywOHDh5/8kI/xyy+/oGPHjvjPf/6jb7tw4UKZ606ePIm7d+/qE/Xhw4fh4uKCevXqwd3d/YmxE9kyrmakam/o0KGoXbs2+vXrh59//hkZGRnYt28fXn/9dfz1118AgIkTJ2L27NnYunUrfv/9d/znP/957GfEAgICEB0djVGjRmHr1q36Pr/88ksAgL+/PwRBwPbt23Ht2jXk5+ejVq1amDp1KiZPnoy1a9fiwoUL+PXXX7Fo0SKsXbsWADBu3Dikp6dj2rRpSEtLw8aNG7FmzRqjnvPvv/9GamqqwXHr1i00bNgQx48fx86dO/HHH39g+vTpOHbsWJn7i4qKMHr0aJw9exbff/89ZsyYgfHjx0OhUBgVO5FNs/ZLO5KXfy4AMeV8VlaWOHz4cLF27dqiUqkU69evL77yyitibm6uKIr3F3xMnDhRVKlUoqurq6jRaMThw4c/cgGIKIri3bt3xcmTJ4s+Pj6ig4OD2KBBA3HVqlX68/Hx8aK3t7coCIIYHR0tiuL9RSvz588Xg4KCRHt7e9HT01OMiIgQ9+/fr79v27ZtYoMGDUSlUil27txZXLVqlVELQACUOdatWycWFhaKI0aMENVqtejq6iq+9tpr4ltvvSW2aNGizO/tvffeEz08PEQXFxfxlVdeEQsLC/XXPCl2LgAhWyaI4iPekBMREdkITjMSEZHNYzIjIiKbx2RGREQ2j8mMiIhsHpMZERHZPCYzIiKyeUxmRERk85jMiIjI5jGZERGRzWMyIyIim8dkRkRENu//ALCIpnPblfKqAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Score"
      ],
      "metadata": {
        "id": "hC9cKMlRCvgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# Load the wine dataset\n",
        "wine_dataset = load_wine()\n",
        "input_features = pd.DataFrame(wine_dataset.data, columns=wine_dataset.feature_names)\n",
        "target_binary = pd.Series(wine_dataset.target % 2)  # Convert to binary target (0 or 1)\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    input_features, target_binary, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize and train the logistic regression model\n",
        "log_reg_model = LogisticRegression(max_iter=1000)\n",
        "log_reg_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_predicted = log_reg_model.predict(X_test)\n",
        "print(\" Precision Score:\", precision_score(y_test, y_predicted))\n",
        "print(\" Recall Score:\", recall_score(y_test, y_predicted))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_predicted))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gn2el-C1GNwm",
        "outputId": "f39e8f49-7818-40a2-cf91-7c5643aa9739"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Precision Score: 0.9333333333333333\n",
            " Recall Score: 1.0\n",
            "F1 Score: 0.9655172413793104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performance"
      ],
      "metadata": {
        "id": "yXLwO76nCvb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create an imbalanced dataset\n",
        "X_data, y_data = make_classification(n_classes=2, weights=[0.9, 0.1], n_samples=1000, random_state=42)\n",
        "df_X = pd.DataFrame(X_data)\n",
        "df_y = pd.Series(y_data)\n",
        "\n",
        "# Split dataset\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(df_X, df_y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with class weights\n",
        "lr_model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "lr_model.fit(X_tr, y_tr)\n",
        "\n",
        "# Predict and print accuracy\n",
        "y_predicted = lr_model.predict(X_te)\n",
        "print(\"Accuracy with Balanced Class Weights:\", accuracy_score(y_te, y_predicted))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NokuHP5GOY-",
        "outputId": "9fdd11c1-42e7-4bc1-9dc9-0671f3af6dce"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Balanced Class Weights: 0.865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance"
      ],
      "metadata": {
        "id": "48VPTrEHCvX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Titanic dataset from the given URL\n",
        "data_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "titanic_df = pd.read_csv(data_url)\n",
        "\n",
        "# Keep only selected columns and remove rows with missing values\n",
        "titanic_df = titanic_df[['Pclass', 'Sex', 'Age', 'Fare', 'Survived']].dropna()\n",
        "\n",
        "# Convert gender from text to numeric values\n",
        "titanic_df['Sex'] = titanic_df['Sex'].map({'male': 0, 'female': 1})\n",
        "\n",
        "# Define input features and target label\n",
        "features = titanic_df[['Pclass', 'Sex', 'Age', 'Fare']]\n",
        "target = titanic_df['Survived']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build and train the logistic regression model\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "predicted_labels = classifier.predict(X_test)\n",
        "print(\"Accuracy of Logistic Regression on Titanic data:\", accuracy_score(y_test, predicted_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHrJsIboGOvy",
        "outputId": "7d4b7aa7-1e8e-45df-d09c-d80ba6957e10"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression on Titanic data: 0.7552447552447552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling"
      ],
      "metadata": {
        "id": "-6jemBIECvT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "dataset = load_breast_cancer()\n",
        "features = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "target = pd.Series(dataset.target)\n",
        "\n",
        "# Split dataset\n",
        "features_train, features_test, target_train, target_test = train_test_split(\n",
        "    features, target, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train model without feature scaling\n",
        "log_reg_raw = LogisticRegression(max_iter=1000)\n",
        "log_reg_raw.fit(features_train, target_train)\n",
        "raw_preds = log_reg_raw.predict(features_test)\n",
        "raw_accuracy = accuracy_score(target_test, raw_preds)\n",
        "\n",
        "# Apply feature scaling\n",
        "scaler = StandardScaler()\n",
        "features_train_scaled = scaler.fit_transform(features_train)\n",
        "features_test_scaled = scaler.transform(features_test)\n",
        "\n",
        "# Train model with scaled features\n",
        "log_reg_scaled = LogisticRegression(max_iter=1000)\n",
        "log_reg_scaled.fit(features_train_scaled, target_train)\n",
        "scaled_preds = log_reg_scaled.predict(features_test_scaled)\n",
        "scaled_accuracy = accuracy_score(target_test, scaled_preds)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(\"Accuracy without Feature Scaling:\", raw_accuracy)\n",
        "print(\"Accuracy with Feature Scaling:\", scaled_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q7g--iyGPFX",
        "outputId": "e839bf57-b182-4882-96fa-aae3dff4117b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Feature Scaling: 0.956140350877193\n",
            "Accuracy with Feature Scaling: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score"
      ],
      "metadata": {
        "id": "swGAbZvZCvP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_breast_cancer()\n",
        "features = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "labels = pd.Series(dataset.target)\n",
        "\n",
        "# Split dataset\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(\n",
        "    features, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(features_train, labels_train)\n",
        "\n",
        "# Predict and compute ROC-AUC score\n",
        "probabilities = classifier.predict_proba(features_test)[:, 1]\n",
        "roc_auc_value = roc_auc_score(labels_test, probabilities)\n",
        "print(\"ROC-AUC Score:\", roc_auc_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LImHjalKGPoL",
        "outputId": "ca8df892-fce6-4eff-c4d7-c0900b4ff9d5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9977071732721913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracy"
      ],
      "metadata": {
        "id": "EIXY-gHaCvJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_breast_cancer()\n",
        "features = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "target = pd.Series(dataset.target)\n",
        "\n",
        "# Split dataset\n",
        "features_train, features_test, target_train, target_test = train_test_split(\n",
        "    features, target, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression with custom learning rate (C=0.5)\n",
        "classifier = LogisticRegression(C=0.5, max_iter=1000)\n",
        "classifier.fit(features_train, target_train)\n",
        "\n",
        "# Predict and print accuracy\n",
        "output = classifier.predict(features_test)\n",
        "print(\"Logistic Regression Accuracy with C=0.5:\", accuracy_score(target_test, output))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7d6XG9wGP4z",
        "outputId": "3a5971a9-f5f9-400a-95a0-09680ebf6b9e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy with C=0.5: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficients"
      ],
      "metadata": {
        "id": "pa9LVdMHCvGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_wine\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "wine_data = load_wine()\n",
        "features_df = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)\n",
        "binary_target = pd.Series(wine_data.target % 2)  # Convert to binary classification\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features_df, binary_target, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance based on absolute coefficient values\n",
        "coeff_importance = np.abs(classifier.coef_[0])\n",
        "columns = features_df.columns\n",
        "top_indices = np.argsort(coeff_importance)[::-1]\n",
        "\n",
        "# Print important features\n",
        "print(\"Top Important Features in Logistic Regression:\")\n",
        "for idx in top_indices[:5]:  # Top 5 features\n",
        "    print(f\"{columns[idx]} - Importance Score: {coeff_importance[idx]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POPNkVGlGQPH",
        "outputId": "f2f3e406-a17d-413d-bc2a-3b337edf7dd0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Important Features in Logistic Regression:\n",
            "color_intensity - Importance Score: 1.8979287441492443\n",
            "malic_acid - Importance Score: 1.1307644272360944\n",
            "alcohol - Importance Score: 0.963540527038184\n",
            "ash - Importance Score: 0.9499402098507542\n",
            "proanthocyanins - Importance Score: 0.5782267693863057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa\n",
        "Score"
      ],
      "metadata": {
        "id": "b2T2eNeWCvC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "features = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
        "labels = pd.Series(wine.target % 2)  # Convert to binary classification\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate Cohen's Kappa Score\n",
        "predicted = classifier.predict(X_test)\n",
        "kappa_score = cohen_kappa_score(y_test, predicted)\n",
        "print(\"Cohen's Kappa Score:\", kappa_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm9969hlGQ4-",
        "outputId": "57ea41bc-732a-42a8-e8df-fd17528b19cd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.9423076923076923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classificatio:"
      ],
      "metadata": {
        "id": "nlqP4f2SCu_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_breast_cancer()\n",
        "features = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "target = pd.Series(dataset.target)\n",
        "\n",
        "# Split dataset\n",
        "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(features_train, target_train)\n",
        "\n",
        "# Get precision-recall values\n",
        "probabilities = classifier.predict_proba(features_test)[:, 1]\n",
        "precision_vals, recall_vals, _ = precision_recall_curve(target_test, probabilities)\n",
        "\n",
        "# Plot the Precision-Recall Curve\n",
        "plt.plot(recall_vals, precision_vals, marker='.')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "0SfsgPpUGRJn",
        "outputId": "2c069675-24cd-4f1f-9118-3ad4af087474"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAARZNJREFUeJzt3XtcVVX+//H3Abl5ATWuGoZo6qSohcrPW2aRKOWkNUlqqZSWt++UjJWYSpdJsilHa0zLr7fp24yYWmNpmGFWXkrz0mR5v6EGKDaAooJw9u8Ph5MnDioEHA779Xw89mM666y99tpL63xm77U+y2IYhiEAAAATcXN2BwAAAKobARAAADAdAiAAAGA6BEAAAMB0CIAAAIDpEAABAADTIQACAACmQwAEAABMhwAIAACYDgEQAIdGjBihsLCwcp2zYcMGWSwWbdiwoUr65OruuOMO3XHHHbbPR48elcVi0eLFi53WJ8CsCICAGmLx4sWyWCy2w9vbW61atdL48eOVlZXl7O7VeCXBRMnh5uamxo0bq1+/ftqyZYuzu1cpsrKyNHHiRLVp00Z169ZVvXr1FBkZqT//+c/KyclxdvcAl1LH2R0AYO/FF19U8+bNdfHiRW3cuFFz587VmjVrtHv3btWtW7fa+jF//nxZrdZynXP77bfrwoUL8vT0rKJeXdvgwYMVGxur4uJi7d+/X2+99ZZ69+6tbdu2KSIiwmn9+q22bdum2NhYnTt3Tg8//LAiIyMlSd9++61eeeUVffnll/r000+d3EvAdRAAATVMv3791KlTJ0nSyJEjdcMNN2jmzJn617/+pcGDBzs8Jz8/X/Xq1avUfnh4eJT7HDc3N3l7e1dqP8rrtttu08MPP2z73LNnT/Xr109z587VW2+95cSeVVxOTo4GDhwod3d37dy5U23atLH7/uWXX9b8+fMr5VpV8XcJqIl4BQbUcHfeeack6ciRI5Iuz82pX7++Dh06pNjYWDVo0EBDhw6VJFmtVs2aNUtt27aVt7e3goKC9MQTT+g///lPqXY/+eQT9erVSw0aNJCvr686d+6sf/zjH7bvHc0BWrp0qSIjI23nREREaPbs2bbvy5oD9P777ysyMlI+Pj7y9/fXww8/rJMnT9rVKbmvkydPasCAAapfv74CAgI0ceJEFRcXV3j8evbsKUk6dOiQXXlOTo6eeuophYaGysvLSy1bttSMGTNKPfWyWq2aPXu2IiIi5O3trYCAAPXt21fffvutrc6iRYt05513KjAwUF5eXrrllls0d+7cCvf5195++22dPHlSM2fOLBX8SFJQUJCmTJli+2yxWPT888+XqhcWFqYRI0bYPpe8dv3iiy80duxYBQYG6sYbb9Ty5ctt5Y76YrFYtHv3blvZ3r179Yc//EGNGzeWt7e3OnXqpFWrVv22mwaqGE+AgBqu5If7hhtusJUVFRUpJiZGPXr00GuvvWZ7NfbEE09o8eLFio+P1x//+EcdOXJEf/vb37Rz505t2rTJ9lRn8eLFevTRR9W2bVslJiaqYcOG2rlzp1JTUzVkyBCH/Vi3bp0GDx6su+66SzNmzJAk7dmzR5s2bdKTTz5ZZv9L+tO5c2clJycrKytLs2fP1qZNm7Rz5041bNjQVre4uFgxMTGKiorSa6+9ps8++0yvv/66WrRooTFjxlRo/I4ePSpJatSoka3s/Pnz6tWrl06ePKknnnhCzZo10+bNm5WYmKiMjAzNmjXLVvexxx7T4sWL1a9fP40cOVJFRUX66quv9PXXX9ue1M2dO1dt27bV73//e9WpU0cfffSRxo4dK6vVqnHjxlWo31datWqVfHx89Ic//OE3t+XI2LFjFRAQoGnTpik/P1/33HOP6tevr2XLlqlXr152dVNSUtS2bVu1a9dOkvTDDz+oe/fuatq0qSZNmqR69epp2bJlGjBggFasWKGBAwdWSZ+B38wAUCMsWrTIkGR89tlnxunTp43jx48bS5cuNW644QbDx8fHOHHihGEYhjF8+HBDkjFp0iS787/66itDkvHee+/ZlaemptqV5+TkGA0aNDCioqKMCxcu2NW1Wq22fx4+fLhx00032T4/+eSThq+vr1FUVFTmPXz++eeGJOPzzz83DMMwCgsLjcDAQKNdu3Z21/r4448NSca0adPsrifJePHFF+3avPXWW43IyMgyr1niyJEjhiTjhRdeME6fPm1kZmYaX331ldG5c2dDkvH+++/b6r700ktGvXr1jP3799u1MWnSJMPd3d1IT083DMMw1q9fb0gy/vjHP5a63pVjdf78+VLfx8TEGOHh4XZlvXr1Mnr16lWqz4sWLbrqvTVq1Mjo0KHDVetcSZKRlJRUqvymm24yhg8fbvtc8neuR48epf5cBw8ebAQGBtqVZ2RkGG5ubnZ/RnfddZcRERFhXLx40VZmtVqNbt26GTfffPN19xmobrwCA2qY6OhoBQQEKDQ0VA899JDq16+vDz74QE2bNrWr9+snIu+//778/Px09913Kzs723ZERkaqfv36+vzzzyVdfpJz9uxZTZo0qdR8HYvFUma/GjZsqPz8fK1bt+667+Xbb7/VqVOnNHbsWLtr3XPPPWrTpo1Wr15d6pzRo0fbfe7Zs6cOHz583ddMSkpSQECAgoOD1bNnT+3Zs0evv/663dOT999/Xz179lSjRo3sxio6OlrFxcX68ssvJUkrVqyQxWJRUlJSqetcOVY+Pj62f87NzVV2drZ69eqlw4cPKzc397r7Xpa8vDw1aNDgN7dTllGjRsnd3d2uLC4uTqdOnbJ7nbl8+XJZrVbFxcVJkn7++WetX79egwYN0tmzZ23jeObMGcXExOjAgQOlXnUCNQWvwIAaZs6cOWrVqpXq1KmjoKAgtW7dWm5u9v9fpU6dOrrxxhvtyg4cOKDc3FwFBgY6bPfUqVOSfnmlVvIK43qNHTtWy5YtU79+/dS0aVP16dNHgwYNUt++fcs859ixY5Kk1q1bl/quTZs22rhxo11ZyRybKzVq1MhuDtPp06ft5gTVr19f9evXt31+/PHH9eCDD+rixYtav3693njjjVJziA4cOKB///vfpa5V4sqxatKkiRo3blzmPUrSpk2blJSUpC1btuj8+fN23+Xm5srPz++q51+Lr6+vzp49+5vauJrmzZuXKuvbt6/8/PyUkpKiu+66S9Ll118dO3ZUq1atJEkHDx6UYRiaOnWqpk6d6rDtU6dOlQregZqAAAioYbp06WKbW1IWLy+vUkGR1WpVYGCg3nvvPYfnlPVjf70CAwO1a9curV27Vp988ok++eQTLVq0SMOGDdOSJUt+U9slfv0UwpHOnTvbAivp8hOfKyf83nzzzYqOjpYk3XvvvXJ3d9ekSZPUu3dv27harVbdfffdeuaZZxxeo+QH/nocOnRId911l9q0aaOZM2cqNDRUnp6eWrNmjf7617+WO5WAI23atNGuXbtUWFj4m1IMlDWZ/MonWCW8vLw0YMAAffDBB3rrrbeUlZWlTZs2afr06bY6Jfc2ceJExcTEOGy7ZcuWFe4vUJUIgIBaokWLFvrss8/UvXt3hz9oV9aTpN27d5f7x8nT01P9+/dX//79ZbVaNXbsWL399tuaOnWqw7ZuuukmSdK+fftsq9lK7Nu3z/Z9ebz33nu6cOGC7XN4ePhV6z/33HOaP3++pkyZotTUVEmXx+DcuXO2QKksLVq00Nq1a/Xzzz+X+RToo48+UkFBgVatWqVmzZrZykteOVaG/v37a8uWLVqxYkWZqRCu1KhRo1KJEQsLC5WRkVGu68bFxWnJkiVKS0vTnj17ZBiG7fWX9MvYe3h4XHMsgZqGOUBALTFo0CAVFxfrpZdeKvVdUVGR7QexT58+atCggZKTk3Xx4kW7eoZhlNn+mTNn7D67ubmpffv2kqSCggKH53Tq1EmBgYGaN2+eXZ1PPvlEe/bs0T333HNd93al7t27Kzo62nZcKwBq2LChnnjiCa1du1a7du2SdHmstmzZorVr15aqn5OTo6KiIknSAw88IMMw9MILL5SqVzJWJU+trhy73NxcLVq0qNz3VpbRo0crJCREf/rTn7R///5S3586dUp//vOfbZ9btGhhm8dU4p133il3OoHo6Gg1btxYKSkpSklJUZcuXexelwUGBuqOO+7Q22+/7TC4On36dLmuB1QnngABtUSvXr30xBNPKDk5Wbt27VKfPn3k4eGhAwcO6P3339fs2bP1hz/8Qb6+vvrrX/+qkSNHqnPnzhoyZIgaNWqk7777TufPny/zddbIkSP1888/684779SNN96oY8eO6c0331THjh31u9/9zuE5Hh4emjFjhuLj49WrVy8NHjzYtgw+LCxMEyZMqMohsXnyySc1a9YsvfLKK1q6dKmefvpprVq1Svfee69GjBihyMhI5efn6/vvv9fy5ct19OhR+fv7q3fv3nrkkUf0xhtv6MCBA+rbt6+sVqu++uor9e7dW+PHj1efPn1sT8aeeOIJnTt3TvPnz1dgYGC5n7iUpVGjRvrggw8UGxurjh072mWC3rFjh/75z3+qa9eutvojR47U6NGj9cADD+juu+/Wd999p7Vr18rf379c1/Xw8ND999+vpUuXKj8/X6+99lqpOnPmzFGPHj0UERGhUaNGKTw8XFlZWdqyZYtOnDih77777rfdPFBVnLkEDcAvSpYkb9u27ar1hg8fbtSrV6/M79955x0jMjLS8PHxMRo0aGBEREQYzzzzjPHTTz/Z1Vu1apXRrVs3w8fHx/D19TW6dOli/POf/7S7zpXL4JcvX2706dPHCAwMNDw9PY1mzZoZTzzxhJGRkWGr8+tl8CVSUlKMW2+91fDy8jIaN25sDB061Las/1r3lZSUZFzPf6pKlpT/5S9/cfj9iBEjDHd3d+PgwYOGYRjG2bNnjcTERKNly5aGp6en4e/vb3Tr1s147bXXjMLCQtt5RUVFxl/+8hejTZs2hqenpxEQEGD069fP2L59u91Ytm/f3vD29jbCwsKMGTNmGAsXLjQkGUeOHLHVq+gy+BI//fSTMWHCBKNVq1aGt7e3UbduXSMyMtJ4+eWXjdzcXFu94uJi49lnnzX8/f2NunXrGjExMcbBgwfLXAZ/tb9z69atMyQZFovFOH78uMM6hw4dMoYNG2YEBwcbHh4eRtOmTY17773XWL58+XXdF+AMFsO4yjNvAACAWog5QAAAwHQIgAAAgOkQAAEAANMhAAIAAKZDAAQAAEyHAAgAAJgOiRAdsFqt+umnn9SgQYOr7o4NAABqDsMwdPbsWTVp0qTUfom/RgDkwE8//aTQ0FBndwMAAFTA8ePHdeONN161DgGQAw0aNJB0eQB9fX2d3BsAAHA98vLyFBoaavsdvxoCIAdKXnv5+voSAAEA4GKuZ/oKk6ABAIDpEAABAADTIQACAACmQwAEAABMhwAIAACYDgEQAAAwHQIgAABgOgRAAADAdAiAAACA6RAAAQAA03FqAPTll1+qf//+atKkiSwWiz788MNrnrNhwwbddttt8vLyUsuWLbV48eJSdebMmaOwsDB5e3srKipKW7durfzOAwAAl+XUACg/P18dOnTQnDlzrqv+kSNHdM8996h3797atWuXnnrqKY0cOVJr16611UlJSVFCQoKSkpK0Y8cOdejQQTExMTp16lRV3Ua5ZORe0OZD2crIvfCbymmr6tty9vVpy7WuT1v82Zu1LVdlMQzDcHYnpMsbl33wwQcaMGBAmXWeffZZrV69Wrt377aVPfTQQ8rJyVFqaqokKSoqSp07d9bf/vY3SZLValVoaKj+53/+R5MmTbquvuTl5cnPz0+5ubmVuhnq4s1H9OJHP8pqSG4W6ZmY1rq3QxN9/N1PenXtvusul1Tuc2jLta5PW651fdriz95MbSXfH6G4zs1+0+9hVSnP77dLBUC33367brvtNs2aNctWtmjRIj311FPKzc1VYWGh6tatq+XLl9u1M3z4cOXk5Ohf//qXw3YLCgpUUFBg+5yXl6fQ0NBKDYAyci+oW/J61YjBBgCggtwtFm2c1Fshfj7O7kop5QmAXGoSdGZmpoKCguzKgoKClJeXpwsXLig7O1vFxcUO62RmZpbZbnJysvz8/GxHaGhopff9SHa+w+DH3eK4flnlHm4Webg5/pK2KqctZ1+ftvjzoi3XuL5Z2yo2DB3NPu/4BBfiUgFQVUlMTFRubq7tOH78eKVfo7l/Pf367527xaKVY7uVq/zLZ3vry2d701YVtuXs69MWf1605RrXN3NbYf515epcKgAKDg5WVlaWXVlWVpZ8fX3l4+Mjf39/ubu7O6wTHBxcZrteXl7y9fW1OypbiJ+Pku+PkLvl8t8kd4tF0+9vpw6hjcpVHuLnQ1tV3Jazr09b/HnRlmtc30xtlcRAFsnWlqtzqTlAzz77rNasWaPvv//eVjZkyBD9/PPPdpOgu3TpojfffFPS5UnQzZo10/jx450+CVq6PBfoaPZ5hfnXtfsLVN5y2qr6tpx9fdpyrevTFn/2tbmt8f/Yro//nanRvcI1qd/vVFO5zCToc+fO6eDBg5KkW2+9VTNnzlTv3r3VuHFjNWvWTImJiTp58qT+/ve/S7q8DL5du3YaN26cHn30Ua1fv15//OMftXr1asXExEi6vAx++PDhevvtt9WlSxfNmjVLy5Yt0969e0vNDSpLVQZAAAC4mmeWf6dl357Q0zGtNa53S2d3p0zl+f2uU019cujbb79V7969bZ8TEhIkXV61tXjxYmVkZCg9Pd32ffPmzbV69WpNmDBBs2fP1o033qj//d//tQU/khQXF6fTp09r2rRpyszMVMeOHZWamnrdwQ8AAKj9aswrsJqEJ0AAAPyiNj4BcqlJ0AAAAJWBAAgAAJgOARAAAKh0NX3/MKdOggYAADXf+cIiSdLZi5dKfZeRe0FHsvPV3L+ebel8yrZ0Ja78vkbvH0YABAAAypSyLV0f//vydlLzvjisS8WGuobfoIIiq748cErLtp2QoctJEvu0DVKThj5avOmobfsnqyFNXrlbt7cKqFEJFAmAAACAQxm5F5S48nu7sgUbj2jBxiOl6hqS1v6QVapc+mX/sJoUADEHCAAAOHQkO19WB8lyWgTUU5vgBg7P6dnS37Z1Rgl3S83bP4wACAAAOFTWRt7/NzJKi+I7O/zu1Qfb65UHImxlbpaauX8YARAAAHCorM1Tr7axaoifj+I6N1Ojuh6SpL8/GlXjJkBLzAECAABXEde5mW5vFeBwk9Srfef+38dDAQ28qr3P14MACAAAXFXJE5/yfleT8QoMAACYDgEQAAAwHQIgAABgOgRAAADAdAiAAABApSv+bwbF02cLnNwTxwiAAABApUrZlq7/nL+8ceqwhd8oZVu6k3tUGgEQAACoNL/eP6xkM9SM3AtO7FVpBEAAAKDSONo/rGQz1JqEAAgAAFSasvYPYzNUAABQa5XsEVaCzVABAIApuMJmqARAAACg0tX0zVAJgAAAgOkQAAEAANMhAAIAAKZDAAQAAEyHAAgAAFQ69gIDAACmwl5gAADAVNgLDAAAmA57gQEAANNhLzAAAGA67AV2nebMmaOwsDB5e3srKipKW7duLbPupUuX9OKLL6pFixby9vZWhw4dlJqaalfn+eefl8VisTvatGlT1bcBAAD+i73AriElJUUJCQlKSkrSjh071KFDB8XExOjUqVMO60+ZMkVvv/223nzzTf34448aPXq0Bg4cqJ07d9rVa9u2rTIyMmzHxo0bq+N2AADAf7EX2FXMnDlTo0aNUnx8vG655RbNmzdPdevW1cKFCx3Wf/fddzV58mTFxsYqPDxcY8aMUWxsrF5//XW7enXq1FFwcLDt8Pf3r47bAQAALsJpAVBhYaG2b9+u6OjoXzrj5qbo6Ght2bLF4TkFBQXy9va2K/Px8Sn1hOfAgQNq0qSJwsPDNXToUKWnXz3/QEFBgfLy8uwOAABQezktAMrOzlZxcbGCgoLsyoOCgpSZmenwnJiYGM2cOVMHDhyQ1WrVunXrtHLlSmVkZNjqREVFafHixUpNTdXcuXN15MgR9ezZU2fPni2zL8nJyfLz87MdoaGhlXOTAACgRnL6JOjymD17tm6++Wa1adNGnp6eGj9+vOLj4+Xm9stt9OvXTw8++KDat2+vmJgYrVmzRjk5OVq2bFmZ7SYmJio3N9d2HD9+vDpuBwCAWoutMMrg7+8vd3d3ZWVl2ZVnZWUpODjY4TkBAQH68MMPlZ+fr2PHjmnv3r2qX7++wsPDy7xOw4YN1apVKx08eLDMOl5eXvL19bU7AABAxbAVxlV4enoqMjJSaWlptjKr1aq0tDR17dr1qud6e3uradOmKioq0ooVK3TfffeVWffcuXM6dOiQQkJCKq3vAADAMbbCuA4JCQmaP3++lixZoj179mjMmDHKz89XfHy8JGnYsGFKTEy01f/mm2+0cuVKHT58WF999ZX69u0rq9WqZ555xlZn4sSJ+uKLL3T06FFt3rxZAwcOlLu7uwYPHlzt9wcAgNm4ylYYdZx58bi4OJ0+fVrTpk1TZmamOnbsqNTUVNvE6PT0dLv5PRcvXtSUKVN0+PBh1a9fX7GxsXr33XfVsGFDW50TJ05o8ODBOnPmjAICAtSjRw99/fXXCggIqO7bAwDAdEq2wrgyCKqJW2FYDMMwrl3NXPLy8uTn56fc3FzmAwEAUE4p29L17IrLr8HcLFLy/RHVkg26PL/fLrUKDAAA1HxshQEAAEyJrTAAAABqGAIgAABgOgRAAADAdAiAAABApStrK4yM3AvafCjb6YkRnZoHCAAA1D6/3grjhd+3U9cWjfXeN+lavOmoDFXv8nhHyAPkAHmAAAComIzcC+r+yvpS2aAdcbdYtHFSb4X4+VTKtckDBAAAnMLRVhiS5OFuKVXmzC0yCIAAAEClKdkK40puFun9J/5fqXJnbpFBAAQAACpNiJ+Pku+PkLvlcrTjbrEo+f4IdWzWWMn3R6gkBrJImn5/u0p7/VVeTIIGAACVKq5zM93eKkBHs88rzL+uLciJ69xMO47lKOXb43rk/93k1C0yCIAAAEClC/Hzcfh0p57X5dCjvrdzQxBegQEAANMhAAIAAKZDAAQAAKpNfkGRJOncxSKn9oMACAAAVIuUbela9u1xSdK7Xx9TyrZ0p/WFAAgAAFS5jNwLSlz5vUpyJBqSJq/c7bQ9wQiAAABAlXOUIZpM0AAAoFZzlCGaTNAAAKBWK8kQXVMyQRMAAQCAahHXuZkGdQqVJKdngiYAAgAA1YZM0AAAAE5CAAQAAEyHAAgAAJgOARAAAKg2bIUBAABMha0wAACAqbAVBgAAMB22wgAAAKbDVhgAAMB02AoDAACYElthAAAAU2IrjP+aM2eOwsLC5O3traioKG3durXMupcuXdKLL76oFi1ayNvbWx06dFBqaupvahMAAJiPUwOglJQUJSQkKCkpSTt27FCHDh0UExOjU6dOOaw/ZcoUvf3223rzzTf1448/avTo0Ro4cKB27txZ4TYBAID5ODUAmjlzpkaNGqX4+HjdcsstmjdvnurWrauFCxc6rP/uu+9q8uTJio2NVXh4uMaMGaPY2Fi9/vrrFW4TAABUH9Nngi4sLNT27dsVHR39S2fc3BQdHa0tW7Y4PKegoEDe3t52ZT4+Ptq4cWOF2yxpNy8vz+4AAACVi0zQkrKzs1VcXKygoCC78qCgIGVmZjo8JyYmRjNnztSBAwdktVq1bt06rVy5UhkZGRVuU5KSk5Pl5+dnO0JDQ3/j3QEAgCuRCfo3mD17tm6++Wa1adNGnp6eGj9+vOLj4+Xm9ttuIzExUbm5ubbj+PHjldRjAAAgkQnaxt/fX+7u7srKyrIrz8rKUnBwsMNzAgIC9OGHHyo/P1/Hjh3T3r17Vb9+fYWHh1e4TUny8vKSr6+v3QEAACoPmaD/y9PTU5GRkUpLS7OVWa1WpaWlqWvXrlc919vbW02bNlVRUZFWrFih++677ze3CQAAqk5NywTt1CxECQkJGj58uDp16qQuXbpo1qxZys/PV3x8vCRp2LBhatq0qZKTkyVJ33zzjU6ePKmOHTvq5MmTev7552W1WvXMM89cd5sAAMA54jo3045jOUr59rjTM0E7NQCKi4vT6dOnNW3aNGVmZqpjx45KTU21TWJOT0+3m99z8eJFTZkyRYcPH1b9+vUVGxurd999Vw0bNrzuNgEAgPPUlEzQFsMwjGtXM5e8vDz5+fkpNzeX+UAAAFSiFz/6UQs3HdHYO1romb5tKrXt8vx+u9QqMAAAgMpAAAQAAKqN6TNBAwAAcyETNAAAMBUyQQMAANMhEzQAADAdMkEDAADTqWmZoAmAAABAtYjr3EyDOoVKktMzQRMAAQCAalNTMkETAAEAANMhAAIAAKZDAAQAAKoNmaABAICpkAkaAACYCpmgAQCA6ZAJGgAAmA6ZoAEAgOmQCRoAAJgSmaABAIApkQkaAADASQiAAABAtSERIgAAMBUSIQIAAFMhESIAADAdEiECAADTIREiAAAwHRIhAgAAUyIRIgAAMCUSIQIAADgJARAAADAdAiAAAFBtyAQNAABMhUzQAADAVMgE/Stz5sxRWFiYvL29FRUVpa1bt161/qxZs9S6dWv5+PgoNDRUEyZM0MWLF23fP//887JYLHZHmzZtqvo2AADAVdS0TNBOXYOWkpKihIQEzZs3T1FRUZo1a5ZiYmK0b98+BQYGlqr/j3/8Q5MmTdLChQvVrVs37d+/XyNGjJDFYtHMmTNt9dq2bavPPvvM9rlOHecutQMAwOxKMkFfGQSZNhP0zJkzNWrUKMXHx+uWW27RvHnzVLduXS1cuNBh/c2bN6t79+4aMmSIwsLC1KdPHw0ePLjUU6M6deooODjYdvj7+1fH7QAAgDKQCfq/CgsLtX37dkVHR//SGTc3RUdHa8uWLQ7P6datm7Zv324LeA4fPqw1a9YoNjbWrt6BAwfUpEkThYeHa+jQoUpPd94kKwAAcFlNygTttHdD2dnZKi4uVlBQkF15UFCQ9u7d6/CcIUOGKDs7Wz169JBhGCoqKtLo0aM1efJkW52oqCgtXrxYrVu3VkZGhl544QX17NlTu3fvVoMGDRy2W1BQoIKCAtvnvLy8SrhDAADwa2SCroANGzZo+vTpeuutt7Rjxw6tXLlSq1ev1ksvvWSr069fPz344INq3769YmJitGbNGuXk5GjZsmVltpucnCw/Pz/bERoaWh23AwAAnMRp4Ze/v7/c3d2VlZVlV56VlaXg4GCH50ydOlWPPPKIRo4cKUmKiIhQfn6+Hn/8cT333HNycysdzzVs2FCtWrXSwYMHy+xLYmKiEhISbJ/z8vIIggAAqAKmT4To6empyMhIpaWl2cqsVqvS0tLUtWtXh+ecP3++VJDj7u4uSTIMw9EpOnfunA4dOqSQkJAy++Ll5SVfX1+7AwAAVC4SIf5XQkKC5s+fryVLlmjPnj0aM2aM8vPzFR8fL0kaNmyYEhMTbfX79++vuXPnaunSpTpy5IjWrVunqVOnqn///rZAaOLEifriiy909OhRbd68WQMHDpS7u7sGDx7slHsEAAA1LxGiU2cgxcXF6fTp05o2bZoyMzPVsWNHpaam2iZGp6en2z3xmTJliiwWi6ZMmaKTJ08qICBA/fv318svv2yrc+LECQ0ePFhnzpxRQECAevTooa+//loBAQHVfn8AAOCyqyVCdMZSeItR1rsjE8vLy5Ofn59yc3N5HQYAQCXIyL2g7q+sL5UIceOk3pUWAJXn99ulVoEBAADXVNMSIVboFVhxcbEWL16stLQ0nTp1Slar1e779evXV0rnAABA7RHXuZl2HMtRyrfHXTMR4pNPPqnFixfrnnvuUbt27WSxWK59EgAAML2akgixQldfunSpli1bVmoLCgAAAFdQoTlAnp6eatmyZWX3BQAA1HIunQjxT3/6k2bPnl1m8kEAAIBfq0mJECv0Cmzjxo36/PPP9cknn6ht27by8PCw+37lypWV0jkAAFA7lJUI8fZWAU5ZCVahAKhhw4YaOHBgZfcFAADUUjUtEWKFAqBFixZVdj8AAEAt1ty/ntwsKpUIMcy/rlP685sSIZ4+fVobN27Uxo0bdfr06crqEwAAqGVqWiLECgVA+fn5evTRRxUSEqLbb79dt99+u5o0aaLHHntM58+fr+w+AgCAWiCuczMN6hQqSU5PhFihACghIUFffPGFPvroI+Xk5CgnJ0f/+te/9MUXX+hPf/pTZfcRAADUEi6dCHHFihVavny57rjjDltZbGysfHx8NGjQIM2dO7ey+gcAAGoRl84DdP78eQUFBZUqDwwM5BUYAABwqCblAapQANS1a1clJSXp4sWLtrILFy7ohRdeUNeuXSutcwAAoHYoKw9QRu4Fp/SnQq/AZs+erZiYGN14443q0KGDJOm7776Tt7e31q5dW6kdBAAArq9W5AFq166dDhw4oPfee0979+6VJA0ePFhDhw6Vj49zlrMBAICaq6blAarwFOy6detq1KhRldkXAABQS5XkAZq04vJrMGfnAbruAGjVqlXq16+fPDw8tGrVqqvW/f3vf/+bOwYAAGqXuM7NtONYjlK+Pe70PEDXHQANGDBAmZmZCgwM1IABA8qsZ7FYVFxcXBl9AwAAtYzL5QGyWq0O/xkAAMDV/Ka9wK6Uk5NTWU0BAIBayqUTIc6YMUMpKSm2zw8++KAaN26spk2b6rvvvqu0zgEAgNrD5RMhzps3T6GhlzczW7dunT777DOlpqaqX79+evrppyu1gwAAwPXVikSImZmZtgDo448/1qBBg9SnTx+FhYUpKiqqUjsIAABcX01LhFihJ0CNGjXS8eOXH2GlpqYqOjpakmQYBivAAABAKSWJEK/kzESIFQqA7r//fg0ZMkR33323zpw5o379+kmSdu7cqZYtW1ZqBwEAgOsrSYRYEgO5TCLEK/31r39VWFiYjh8/rldffVX169eXJGVkZGjs2LGV2kEAAFA7uGQixCt5eHho4sSJpconTJjwmzsEAABqL5dLhMhWGAAA4LeqKXmALIZhGNeuJrm5udm2wnBzK3vqUG3YCiMvL09+fn7Kzc2Vr6+vs7sDAECtkLIt3W4z1FceiKjU12Dl+f2+7knQVqtVgYGBtn8u63D14AcAAFS+mpYHqNK2wgAAACjL1fIAOUOFAqA//vGPeuONN0qV/+1vf9NTTz31W/sEAABqmVqRB2jFihXq3r17qfJu3bpp+fLl5Wprzpw5CgsLk7e3t6KiorR169ar1p81a5Zat24tHx8fhYaGasKECbp48eJvahMAAFStmpYHqEIB0JkzZ+Tn51eq3NfXV9nZ2dfdTkpKihISEpSUlKQdO3aoQ4cOiomJ0alTpxzW/8c//qFJkyYpKSlJe/bs0YIFC5SSkqLJkydXuE0AAFA94jo306BOl7fScnYeoAoFQC1btlRqamqp8k8++UTh4eHX3c7MmTM1atQoxcfH65ZbbtG8efNUt25dLVy40GH9zZs3q3v37hoyZIjCwsLUp08fDR482O4JT3nbBAAA1cfl8gBdKSEhQePHj9fp06d15513SpLS0tL0+uuva9asWdfVRmFhobZv367ExERbmZubm6Kjo7VlyxaH53Tr1k3/93//p61bt6pLly46fPiw1qxZo0ceeaTCbUpSQUGBCgoKbJ/z8vKu6x4AAED51JQ8QBUKgB599FEVFBTo5Zdf1ksvvSRJCgsL09y5czVs2LDraiM7O1vFxcUKCgqyKw8KCtLevXsdnjNkyBBlZ2erR48eMgxDRUVFGj16tO0VWEXalKTk5GS98MIL19VvAABQMSnb0rXs28ubqb/79TG1berrtNdgFV4GP2bMGJ04cUJZWVnKy8vT4cOHrzv4qagNGzZo+vTpeuutt7Rjxw6tXLlSq1evtgVhFZWYmKjc3FzbUbLTPQAAqBw1LQ9QhV/AFRUVacOGDTp06JCGDBkiSfrpp5/k6+tr2xz1avz9/eXu7q6srCy78qysLAUHBzs8Z+rUqXrkkUc0cuRISVJERITy8/P1+OOP67nnnqtQm5Lk5eUlLy+va/YZAABUzNXyADljJViFngAdO3ZMERERuu+++zRu3DidPn1akjRjxgyHm6Q64unpqcjISKWlpdnKrFar0tLS1LVrV4fnnD9/vtQ2HO7u7pIkwzAq1CYAAKh6tSIP0JNPPqlOnTrpP//5j3x8fonaBg4caBd8XEtCQoLmz5+vJUuWaM+ePRozZozy8/MVHx8vSRo2bJjdhOb+/ftr7ty5Wrp0qY4cOaJ169Zp6tSp6t+/vy0QulabAACg+tW0PEAVegX21VdfafPmzfL09LQrDwsL08mTJ6+7nbi4OJ0+fVrTpk1TZmamOnbsqNTUVNsk5vT0dLsnPlOmTJHFYtGUKVN08uRJBQQEqH///nr55Zevu00AAOAccZ2bacexHKV8e9zpeYCuezf4KzVq1EibNm3SLbfcogYNGui7775TeHi4Nm7cqAceeKDUHBxXw27wAABUjRc/+lELNx3R2Dta6Jm+bSq17SrZDf5Kffr0scv3Y7FYdO7cOSUlJSk2NrYiTQIAABOoKXmAKhQAvfbaa7YnQBcvXrRlZj558qRmzJhR2X0EAAC1wK/zAKVsS3daXyr0Cky6vAw+JSVF3333nc6dO6fbbrtNQ4cOtZsU7ap4BQYAQOXKyL2g7q+st1sK726xaOOk3pU2Ebo8v9/lngR96dIltWnTRh9//LGGDh2qoUOHVrijAADAHFw+D5CHh4cuXrxYFX0BAAC1VK3IAzRu3DjNmDFDRUXOncAEAABcQ63IA7Rt2zalpaXp008/VUREhOrVq2f3/cqVKyulcwAAoPaoSXmAKhQANWzYUA888EBl9wUAANRy9bwuhx71vSu8HWmlKNfVrVar/vKXv2j//v0qLCzUnXfeqeeff75WrPwCAABVzyXzAL388suaPHmy6tevr6ZNm+qNN97QuHHjqqpvAACgFqlJeYDKFQD9/e9/11tvvaW1a9fqww8/1EcffaT33ntPVqu1qvoHAABqgYzcC0pc+b1KVsIbkiav3K2M3AtO6U+5AqD09HS7rS6io6NlsVj0008/VXrHAABA7XG1PEDOUK4AqKioSN7e3nZlHh4eunTpUqV2CgAA1C41LQ9QuSZBG4ahESNGyMvLy1Z28eJFjR492m4pPMvgAQDAlUryAE1acfk1mEvlARo+fHipsocffrjSOgMAAGovl80DtGjRoqrqBwAAQLWp0FYYAAAA5eWyy+ABAAAqwqWXwQMAAFSESy+DBwAAqIiatgyeAAgAAFS5kmXwJTGQs5fBEwABAIBqEde5mQZ1CpUkpy+DJwACAADVpp7X5Qw89b3LlYmn0hEAAQCAapNfUCRJOnexyKn9IAACAADVgjxAAADAVMgDBAAATIc8QAAAwHTIAwQAAEyHPEAAAMCUyAMEAADgRARAAACgWrAMHgAAmArL4B2YM2eOwsLC5O3traioKG3durXMunfccYcsFkup45577rHVGTFiRKnv+/btWx23AgAAHKhpy+CduxGHpJSUFCUkJGjevHmKiorSrFmzFBMTo3379ikwMLBU/ZUrV6qwsND2+cyZM+rQoYMefPBBu3p9+/bVokWLbJ+9vLyq7iYAAMBVlSyDvzIIMvUy+JkzZ2rUqFGKj4/XLbfconnz5qlu3bpauHChw/qNGzdWcHCw7Vi3bp3q1q1bKgDy8vKyq9eoUaPquB0AAOAAy+CvUFhYqO3btys6OtpW5ubmpujoaG3ZsuW62liwYIEeeugh1atXz658w4YNCgwMVOvWrTVmzBidOXOmUvsOAADKpyYtg3fqK7Ds7GwVFxcrKCjIrjwoKEh79+695vlbt27V7t27tWDBArvyvn376v7771fz5s116NAhTZ48Wf369dOWLVvk7u5eqp2CggIVFBTYPufl5VXwjgAAgCtw+hyg32LBggWKiIhQly5d7Mofeugh2z9HRESoffv2atGihTZs2KC77rqrVDvJycl64YUXqry/AACY2a+Xwbdt6uu0p0BOfQXm7+8vd3d3ZWVl2ZVnZWUpODj4qufm5+dr6dKleuyxx655nfDwcPn7++vgwYMOv09MTFRubq7tOH78+PXfBAAAuCaWwV/B09NTkZGRSktLs5VZrValpaWpa9euVz33/fffV0FBgR5++OFrXufEiRM6c+aMQkJCHH7v5eUlX19fuwMAAFSemrYM3umrwBISEjR//nwtWbJEe/bs0ZgxY5Sfn6/4+HhJ0rBhw5SYmFjqvAULFmjAgAG64YYb7MrPnTunp59+Wl9//bWOHj2qtLQ03XfffWrZsqViYmKq5Z4AAIC9mrYbvNPnAMXFxen06dOaNm2aMjMz1bFjR6WmptomRqenp8vNzT5O27dvnzZu3KhPP/20VHvu7u7697//rSVLlignJ0dNmjRRnz599NJLL5ELCAAAJylZBj9pxeXXYM5eBm8xDMO4djVzycvLk5+fn3Jzc3kdBgBAJXp2+b+V8u1xDft/N+nFAe0qte3y/H47/RUYAABAdSMAAgAA1YLd4AEAgKmwDB4AAJgOy+ABAIDp1LRl8ARAAACgyrEbPAAAMKWatBs8ARAAADAdAiAAAFAtWAYPAABMhWXwAADAdFgGDwAATIdl8AAAwHRYBg8AAEwprnMz3ds+RJJ0/61NWQYPAABqv5Rt6fr43xmSpJU7T7IKDAAA1G6sAgMAAKbDKjAAAGA6rAIDAACmwyowAABgSmyGCgAA4EQEQAAAoFqwGSoAADAVlsEDAADTYRk8AAAwHZbBAwAA02EZPAAAMCU2QwUAAKbDZqgAAMBUWAUGAABMh1VgAADAdFgFBgAATIdVYAAAwJTYDBUAAMCJakQANGfOHIWFhcnb21tRUVHaunVrmXXvuOMOWSyWUsc999xjq2MYhqZNm6aQkBD5+PgoOjpaBw4cqI5bAQAAZWAz1CukpKQoISFBSUlJ2rFjhzp06KCYmBidOnXKYf2VK1cqIyPDduzevVvu7u568MEHbXVeffVVvfHGG5o3b56++eYb1atXTzExMbp48WJ13RYAALgCy+B/ZebMmRo1apTi4+N1yy23aN68eapbt64WLlzosH7jxo0VHBxsO9atW6e6devaAiDDMDRr1ixNmTJF9913n9q3b6+///3v+umnn/Thhx9W450BAIASLIO/QmFhobZv367o6GhbmZubm6Kjo7Vly5bramPBggV66KGHVK9ePUnSkSNHlJmZademn5+foqKiymyzoKBAeXl5dgcAAKg8LIO/QnZ2toqLixUUFGRXHhQUpMzMzGuev3XrVu3evVsjR460lZWcV542k5OT5efnZztCQ0PLeysAAOAqWAZfiRYsWKCIiAh16dLlN7WTmJio3Nxc23H8+PFK6iEAACjBZqj/5e/vL3d3d2VlZdmVZ2VlKTg4+Krn5ufna+nSpXrsscfsykvOK0+bXl5e8vX1tTsAAEDlYjPU//L09FRkZKTS0tJsZVarVWlpaeratetVz33//fdVUFCghx9+2K68efPmCg4OtmszLy9P33zzzTXbBAAAVaOmrQKr45SrXiEhIUHDhw9Xp06d1KVLF82aNUv5+fmKj4+XJA0bNkxNmzZVcnKy3XkLFizQgAEDdMMNN9iVWywWPfXUU/rzn/+sm2++Wc2bN9fUqVPVpEkTDRgwoLpuCwAAXOFqq8CcMQ/I6QFQXFycTp8+rWnTpikzM1MdO3ZUamqqbRJzenq63NzsH1Tt27dPGzdu1KeffuqwzWeeeUb5+fl6/PHHlZOTox49eig1NVXe3t5Vfj8AAKC0klVgVwZBzlwFZjEMw7h2NXPJy8uTn5+fcnNzmQ8EAEAlSdmWrkkrLr8Gs0h65YGISp0IXZ7fb5deBQYAAFwHq8AAAIDpsAoMAACYSk1bBUYABAAAqhx7gQEAANNhLzAAAGA67AUGAABMiVVgAADAdFgFBgAATIVVYAAAwHRYBQYAAEyHVWAAAMB0WAUGAABMy/jV/zoLARAAAKhyJZOgr8QkaAAAUKsxCRoAAJgOk6ABAIDpMAkaAACYElthAAAA02ErDAAAYCpshQEAAEyHVWAAAMB0WAUGAABMh1VgAADAlFgFBgAATIdVYAAAwFRYBQYAAEyHVWAAAMB0WAUGAABMh1VgAADAtIxf/a+zEAABAIAqVzIJ+kpMggYAALUak6ABAIDpMAn6V+bMmaOwsDB5e3srKipKW7duvWr9nJwcjRs3TiEhIfLy8lKrVq20Zs0a2/fPP/+8LBaL3dGmTZuqvg0AAHAVNW0SdB2nXPW/UlJSlJCQoHnz5ikqKkqzZs1STEyM9u3bp8DAwFL1CwsLdffddyswMFDLly9X06ZNdezYMTVs2NCuXtu2bfXZZ5/ZPtep49TbBAAA/1VTJkE7NTKYOXOmRo0apfj4eEnSvHnztHr1ai1cuFCTJk0qVX/hwoX6+eeftXnzZnl4eEiSwsLCStWrU6eOgoODq7TvAADg+pU1Cfr2VgFOeQrktFdghYWF2r59u6Kjo3/pjJuboqOjtWXLFofnrFq1Sl27dtW4ceMUFBSkdu3aafr06SouLrard+DAATVp0kTh4eEaOnSo0tOdt9cIAACoeZOgnfYEKDs7W8XFxQoKCrIrDwoK0t69ex2ec/jwYa1fv15Dhw7VmjVrdPDgQY0dO1aXLl1SUlKSJCkqKkqLFy9W69atlZGRoRdeeEE9e/bU7t271aBBA4ftFhQUqKCgwPY5Ly+vku4SAABIv0yCvjIIMvUk6PKwWq0KDAzUO++8o8jISMXFxem5557TvHnzbHX69eunBx98UO3bt1dMTIzWrFmjnJwcLVu2rMx2k5OT5efnZztCQ0Or43YAADCNmjYJ2mkBkL+/v9zd3ZWVlWVXnpWVVeb8nZCQELVq1Uru7u62st/97nfKzMxUYWGhw3MaNmyoVq1a6eDBg2X2JTExUbm5ubbj+PHjFbgjAABwNXGdm+ne9iGSpPtvbaq4zs2c1henBUCenp6KjIxUWlqarcxqtSotLU1du3Z1eE737t118OBBWa1WW9n+/fsVEhIiT09Ph+ecO3dOhw4dUkhISJl98fLykq+vr90BAAAqV8q2dH387wxJ0sqdJ5WyzXlzdJ36CiwhIUHz58/XkiVLtGfPHo0ZM0b5+fm2VWHDhg1TYmKirf6YMWP0888/68knn9T+/fu1evVqTZ8+XePGjbPVmThxor744gsdPXpUmzdv1sCBA+Xu7q7BgwdX+/0BAIDLSlaBXbkM3plbYTh1GXxcXJxOnz6tadOmKTMzUx07dlRqaqptYnR6errc3H6J0UJDQ7V27VpNmDBB7du3V9OmTfXkk0/q2WeftdU5ceKEBg8erDNnziggIEA9evTQ119/rYCAgGq/PwAAcNnVVoE5Yx6QxTAMZ+ciqnHy8vLk5+en3NxcXocBAFAJMnIvqPsr60utAts4qXelBUDl+f12qVVgAADANbEKDAAAmFZN2QqDAAgAAFS5srbCcNYkaAIgAABQ5WraVhgEQAAAoMqVbIVxJbbCAAAAtRqToAEAgGkxCRoAAJgGk6ABAIDpMAkaAACYDpOgAQCA6TAJGgAAmBaToAEAgGkwCRoAAJgOk6ABAIDpMAkaAACYDpOgAQCAaTEJGgAAmAaToAEAgOkwCRoAAJgOk6ABAIDplEyCdrdcjoLcLRanToKu45SrAgAA04nr3Ey3twrQ0ezzCvOv67TgRyIAAgAA1SjEz8epgU8JXoEBAADTIQACAACmQwAEAABMhwAIAACYDgEQAAAwHQIgAABgOgRAAADAdAiAAACA6RAAAQAA0yEAAgAApkMABAAATIe9wBwwDEOSlJeX5+SeAACA61Xyu13yO341BEAOnD17VpIUGhrq5J4AAIDyOnv2rPz8/K5ax2JcT5hkMlarVT/99JMaNGggi8VSqW3n5eUpNDRUx48fl6+vb6W2jV8wztWDca4ejHP1YJyrR1WOs2EYOnv2rJo0aSI3t6vP8uEJkANubm668cYbq/Qavr6+/AtWDRjn6sE4Vw/GuXowztWjqsb5Wk9+SjAJGgAAmA4BEAAAMB0CoGrm5eWlpKQkeXl5ObsrtRrjXD0Y5+rBOFcPxrl61JRxZhI0AAAwHZ4AAQAA0yEAAgAApkMABAAATIcACAAAmA4BUBWYM2eOwsLC5O3traioKG3duvWq9d9//321adNG3t7eioiI0Jo1a6qpp66tPOM8f/589ezZU40aNVKjRo0UHR19zT8XXFbev88lli5dKovFogEDBlRtB2uJ8o5zTk6Oxo0bp5CQEHl5ealVq1b8t+M6lHecZ82apdatW8vHx0ehoaGaMGGCLl68WE29dU1ffvml+vfvryZNmshisejDDz+85jkbNmzQbbfdJi8vL7Vs2VKLFy+u8n7KQKVaunSp4enpaSxcuND44YcfjFGjRhkNGzY0srKyHNbftGmT4e7ubrz66qvGjz/+aEyZMsXw8PAwvv/++2ruuWsp7zgPGTLEmDNnjrFz505jz549xogRIww/Pz/jxIkT1dxz11LecS5x5MgRo2nTpkbPnj2N++67r3o668LKO84FBQVGp06djNjYWGPjxo3GkSNHjA0bNhi7du2q5p67lvKO83vvvWd4eXkZ7733nnHkyBFj7dq1RkhIiDFhwoRq7rlrWbNmjfHcc88ZK1euNCQZH3zwwVXrHz582Khbt66RkJBg/Pjjj8abb75puLu7G6mpqVXaTwKgStalSxdj3Lhxts/FxcVGkyZNjOTkZIf1Bw0aZNxzzz12ZVFRUcYTTzxRpf10deUd518rKioyGjRoYCxZsqSqulgrVGSci4qKjG7duhn/+7//awwfPpwA6DqUd5znzp1rhIeHG4WFhdXVxVqhvOM8btw4484777QrS0hIMLp3716l/axNricAeuaZZ4y2bdvalcXFxRkxMTFV2DPD4BVYJSosLNT27dsVHR1tK3Nzc1N0dLS2bNni8JwtW7bY1ZekmJiYMuujYuP8a+fPn9elS5fUuHHjquqmy6voOL/44osKDAzUY489Vh3ddHkVGedVq1apa9euGjdunIKCgtSuXTtNnz5dxcXF1dVtl1ORce7WrZu2b99ue012+PBhrVmzRrGxsdXSZ7Nw1u8gm6FWouzsbBUXFysoKMiuPCgoSHv37nV4TmZmpsP6mZmZVdZPV1eRcf61Z599Vk2aNCn1Lx1+UZFx3rhxoxYsWKBdu3ZVQw9rh4qM8+HDh7V+/XoNHTpUa9as0cGDBzV27FhdunRJSUlJ1dFtl1ORcR4yZIiys7PVo0cPGYahoqIijR49WpMnT66OLptGWb+DeXl5unDhgnx8fKrkujwBgum88sorWrp0qT744AN5e3s7uzu1xtmzZ/XII49o/vz58vf3d3Z3ajWr1arAwEC98847ioyMVFxcnJ577jnNmzfP2V2rVTZs2KDp06frrbfe0o4dO7Ry5UqtXr1aL730krO7hkrAE6BK5O/vL3d3d2VlZdmVZ2VlKTg42OE5wcHB5aqPio1ziddee02vvPKKPvvsM7Vv374qu+nyyjvOhw4d0tGjR9W/f39bmdVqlSTVqVNH+/btU4sWLaq20y6oIn+fQ0JC5OHhIXd3d1vZ7373O2VmZqqwsFCenp5V2mdXVJFxnjp1qh555BGNHDlSkhQREaH8/Hw9/vjjeu655+TmxjOEylDW76Cvr2+VPf2ReAJUqTw9PRUZGam0tDRbmdVqVVpamrp27erwnK5du9rVl6R169aVWR8VG2dJevXVV/XSSy8pNTVVnTp1qo6uurTyjnObNm30/fffa9euXbbj97//vXr37q1du3YpNDS0OrvvMiry97l79+46ePCgLcCUpP379yskJITgpwwVGefz58+XCnJKgk6DbTQrjdN+B6t0irUJLV261PDy8jIWL15s/Pjjj8bjjz9uNGzY0MjMzDQMwzAeeeQRY9KkSbb6mzZtMurUqWO89tprxp49e4ykpCSWwV+H8o7zK6+8Ynh6ehrLly83MjIybMfZs2eddQsuobzj/GusArs+5R3n9PR0o0GDBsb48eONffv2GR9//LERGBho/PnPf3bWLbiE8o5zUlKS0aBBA+Of//yncfjwYePTTz81WrRoYQwaNMhZt+ASzp49a+zcudPYuXOnIcmYOXOmsXPnTuPYsWOGYRjGpEmTjEceecRWv2QZ/NNPP23s2bPHmDNnDsvgXdWbb75pNGvWzPD09DS6dOlifP3117bvevXqZQwfPtyu/rJly4xWrVoZnp6eRtu2bY3Vq1dXc49dU3nG+aabbjIklTqSkpKqv+Muprx/n69EAHT9yjvOmzdvNqKiogwvLy8jPDzcePnll42ioqJq7rXrKc84X7p0yXj++eeNFi1aGN7e3kZoaKgxduxY4z//+U/1d9yFfP755w7/e1sytsOHDzd69epV6pyOHTsanp6eRnh4uLFo0aIq76fFMHiOBwAAzIU5QAAAwHQIgAAAgOkQAAEAANMhAAIAAKZDAAQAAEyHAAgAAJgOARAAADAdAiAAuE4Wi0UffvihJOno0aOyWCzatWuXU/sEoGIIgAC4hBEjRshischiscjDw0PNmzfXM888o4sXLzq7awBcELvBA3AZffv21aJFi3Tp0iVt375dw4cPl8Vi0YwZM5zdNQAuhidAAFyGl5eXgoODFRoaqgEDBig6Olrr1q2TdHln7+TkZDVv3lw+Pj7q0KGDli9fbnf+Dz/8oHvvvVe+vr5q0KCBevbsqUOHDkmStm3bprvvvlv+/v7y8/NTr169tGPHjmq/RwDVgwAIgEvavXu3Nm/eLE9PT0lScnKy/v73v2vevHn64YcfNGHCBD388MP64osvJEknT57U7bffLi8vL61fv17bt2/Xo48+qqKiIknS2bNnNXz4cG3cuFFff/21br75ZsXGxurs2bNOu0cAVYdXYABcxscff6z69eurqKhIBQUFcnNz09/+9jcVFBRo+vTp+uyzz9S1a1dJUnh4uDZu3Ki3335bvXr10pw5c+Tn56elS5fKw8NDktSqVStb23feeafdtd555x01bNhQX3zxhe69997qu0kA1YIACIDL6N27t+bOnav8/Hz99a9/VZ06dfTAAw/ohx9+0Pnz53X33Xfb1S8sLNStt94qSdq1a5d69uxpC35+LSsrS1OmTNGGDRt06tQpFRcX6/z580pPT6/y+wJQ/QiAALiMevXqqWXLlpKkhQsXqkOHDlqwYIHatWsnSVq9erWaNm1qd46Xl5ckycfH56ptDx8+XGfOnNHs2bN10003ycvLS127dlVhYWEV3AkAZyMAAuCS3NzcNHnyZCUkJGj//v3y8vJSenq6evXq5bB++/bttWTJEl26dMnhU6BNmzbprbfeUmxsrCTp+PHjys7OrtJ7AOA8TIIG4LIefPBBubu76+2339bEiRM1YcIELVmyRIcOHdKOHTv05ptvasmSJZKk8ePHKy8vTw899JC+/fZbHThwQO+++6727dsnSbr55pv17rvvas+ePfrmm280dOjQaz41AuC6eAIEwGXVqVNH48eP16uvvqojR44oICBAycnJOnz4sBo2bKjbbrtNkydPliTdcMMNWr9+vZ5++mn16tVL7u7u6tixo7p37y5JWrBggR5//HHddtttCg0N1fTp0zVx4kRn3h6AKmQxDMNwdicAAACqE6/AAACA6RAAAQAA0yEAAgAApkMABAAATIcACAAAmA4BEAAAMB0CIAAAYDoEQAAAwHQIgAAAgOkQAAEAANMhAAIAAKZDAAQAAEzn/wOyGyN0eVk0mwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracy"
      ],
      "metadata": {
        "id": "zzYKr3j8Cu8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "digit_data = load_digits()\n",
        "features = pd.DataFrame(digit_data.data)\n",
        "labels = pd.Series(digit_data.target % 2)  # Binary classification\n",
        "\n",
        "# Split dataset\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(\n",
        "    features, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "solver_list = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Train and evaluate models with different solvers\n",
        "for solver_name in solver_list:\n",
        "    classifier = LogisticRegression(solver=solver_name, max_iter=1000)\n",
        "    classifier.fit(features_train, labels_train)\n",
        "    label_preds = classifier.predict(features_test)\n",
        "    acc_score = accuracy_score(labels_test, label_preds)\n",
        "    print(f\"Accuracy with {solver_name} solver:\", acc_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5AVNIHNGRYi",
        "outputId": "abd93da6-ff99-4733-e8b9-04fc7303241e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with liblinear solver: 0.9305555555555556\n",
            "Accuracy with saga solver: 0.9305555555555556\n",
            "Accuracy with lbfgs solver: 0.9305555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC)"
      ],
      "metadata": {
        "id": "uSl_wCB4Cu4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.metrics import matthews_corrcoef  # Still from sklearn.metrics\n",
        "\n",
        "# Load digits dataset\n",
        "digits_data = load_digits()\n",
        "X = pd.DataFrame(digits_data.data)\n",
        "y = pd.Series(digits_data.target % 2)  # Convert to binary classification (even vs odd)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate with Matthews Correlation Coefficient\n",
        "mcc_score = matthews_corrcoef(y_test, y_pred)\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fn__Jc_tGRq0",
        "outputId": "ef7e5c74-f615-4d85-be50-01c245379d0a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.8614691244572492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scaling"
      ],
      "metadata": {
        "id": "Vd5BHrpJCuzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset\n",
        "cancer_data = load_breast_cancer()\n",
        "features = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)\n",
        "labels = pd.Series(cancer_data.target)\n",
        "\n",
        "# Split dataset\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(\n",
        "    features, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression on raw data\n",
        "logreg_raw = LogisticRegression(max_iter=1000)\n",
        "logreg_raw.fit(features_train, labels_train)\n",
        "raw_predictions = logreg_raw.predict(features_test)\n",
        "raw_accuracy = accuracy_score(labels_test, raw_predictions)\n",
        "\n",
        "# Apply feature scaling\n",
        "scaler_obj = StandardScaler()\n",
        "features_train_scaled = scaler_obj.fit_transform(features_train)\n",
        "features_test_scaled = scaler_obj.transform(features_test)\n",
        "\n",
        "# Train Logistic Regression on scaled data\n",
        "logreg_scaled = LogisticRegression(max_iter=1000)\n",
        "logreg_scaled.fit(features_train_scaled, labels_train)\n",
        "scaled_predictions = logreg_scaled.predict(features_test_scaled)\n",
        "scaled_accuracy = accuracy_score(labels_test, scaled_predictions)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(\"Accuracy on Raw Data:\", raw_accuracy)\n",
        "print(\"Accuracy on Scaled Data:\", scaled_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEoR01GnGSIO",
        "outputId": "fe8acf3b-080e-43c7-d69e-512f618311fc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Raw Data: 0.956140350877193\n",
            "Accuracy on Scaled Data: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validation"
      ],
      "metadata": {
        "id": "9yN69ALwCus2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "digit_data = load_digits()\n",
        "feature_set = pd.DataFrame(digit_data.data)\n",
        "binary_labels = pd.Series(digit_data.target % 2)  # Convert to binary classification\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    feature_set, binary_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define hyperparameter grid\n",
        "hyperparams = {'C': np.logspace(-4, 4, 10)}\n",
        "\n",
        "# Apply GridSearchCV to find optimal C value\n",
        "grid_model = GridSearchCV(LogisticRegression(max_iter=1000), hyperparams, cv=5)\n",
        "grid_model.fit(X_train, y_train)\n",
        "\n",
        "# Best model and accuracy\n",
        "optimal_model = grid_model.best_estimator_\n",
        "final_predictions = optimal_model.predict(X_test)\n",
        "\n",
        "print(\"Best Regularization Parameter (C):\", grid_model.best_params_['C'])\n",
        "print(\"Best Model Accuracy:\", accuracy_score(y_test, final_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49EufkCIGSba",
        "outputId": "76ba2b29-826f-4aac-923f-af83b25f98dd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Regularization Parameter (C): 0.005994842503189409\n",
            "Best Model Accuracy: 0.9361111111111111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions"
      ],
      "metadata": {
        "id": "gDLR3Wi7CulL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "breast_cancer_data = load_breast_cancer()\n",
        "features = pd.DataFrame(breast_cancer_data.data, columns=breast_cancer_data.feature_names)\n",
        "target = pd.Series(breast_cancer_data.target)\n",
        "\n",
        "# Split dataset\n",
        "features_train, features_test, target_train, target_test = train_test_split(\n",
        "    features, target, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression\n",
        "logreg_model = LogisticRegression(max_iter=1000)\n",
        "logreg_model.fit(features_train, target_train)\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(logreg_model, \"logistic_model.pkl\")\n",
        "\n",
        "# Load the model and make predictions\n",
        "loaded_logreg_model = joblib.load(\"logistic_model.pkl\")\n",
        "predicted_labels = loaded_logreg_model.predict(features_test)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Logistic Regression Accuracy after Loading Model:\", accuracy_score(target_test, predicted_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27e5RHJ7GTFe",
        "outputId": "601b7353-5557-41a8-b1a4-0cb371fe9b32"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy after Loading Model: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "siYkqeDsCuSN"
      }
    }
  ]
}